import torch
import cv2
import time
import os
import sys
import numpy as np
import json
import asyncio
import base64
import logging
from PIL import Image
from torchvision import transforms, models
import torch.nn as nn
from asyncua import ua
from asyncua.client import Client as AsyncuaClient
from pymycobot import MyCobot320
from db_manager import DBManager

EXECUTE_MISSION_COUNT = 0
LOAD_OBJECT_COUNT = 2

# --- ë¡œê¹… ì„¤ì • ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("RobotArmMain")

# --- AI ëª¨ë¸ ë° ë¹„ì „ ì„¤ì • ---
CLASS_NAMES = ["ESP32", "L298N", "MB102"]
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MODEL_CLS_PATH = "best_trck_obj_cls_model.pth"
MODEL_RZ_PATH = "best_trck_coords_tracking_model.pth"
RZ_CENTERS = np.arange(-90 + 5, 70 + 5 + 1e-6, 10, dtype=np.float32)

# --- í•˜ë“œì›¨ì–´ ì œì–´ íŒŒë¼ë¯¸í„° ---
PORT, BAUD = "COM3", 115200
MOVEMENT_SPEED = 70
PICK_Z_HEIGHT = 260
GRIPPER_SPEED = 50
GRIPPER_OPEN, GRIPPER_CLOSE = 85, 25
GRIPPER_DELAY = 1.0

# --- ë¹„ì „ ì¢Œí‘œ ë³´ì • (Pixel to MM) ---
CAMERA_INDEX = 0
TARGET_CENTER_U, TARGET_CENTER_V = 320, 180
PIXEL_TO_MM_X, PIXEL_TO_MM_Y = 0.526, -0.698

# --- ë¡œë´‡ ì£¼ìš” í¬ì¦ˆ (Angles & Coords) ---
CONVEYOR_CAPTURE_POSE = [0, 0, 90, 0, -90, -90]
ROBOTARM_CAPTURE_POSE = [0, 0, 10, 80, -90, 90]
INTERMEDIATE_POSE = [-17.2, 30.49, 4.48, 53.08, -90.87, -85.86]
BASE_PICK_COORDS = [-237.90, 20, 183.6, -174.98, 0, 0]
GLOBAL_TARGET_COORDS = [-114, -195, 250, 177.71, 0.22, 0]
GLOBAL_TARGET_TMP_COORDS = [-150.0, -224.4, 318.1, 176.26, 3.2, 3.02]

# --- OPC UA ì„¤ì • ---
OPCUA_SERVER_URL = "opc.tcp://172.30.1.61:4840/freeopcua/server/"
READ_METHOD_NODE = "ns=2;s=read_arm_go_move"

WRITE_OBJ_NODE = "ns=2;i=3"
WRITE_METHOD_NODE = "ns=2;s=write_send_arm_json"

WRITE_SINGLE_OBJ_NODE = "ns=2;i=3"
WRITE_SINGLE_METHOD_NODE = "ns=2;s=write_arm_place_single"

WRITE_COMPLETE_OBJ_NODE = "ns=2;i=3"
WRITE_COMPLETE_METHOD_NODE = "ns=2;s=write_arm_place_completed"

LOWER_RED_HSV1 = np.array([0, 100, 100])
UPPER_RED_HSV1 = np.array([15, 255, 255])
LOWER_RED_HSV2 = np.array([155, 100, 100])
UPPER_RED_HSV2 = np.array([179, 255, 255])
#

class ResNetMultiTask(nn.Module):
    """Rz ì¶”ë¡ ì„ ìœ„í•œ Multi-Task ResNet50 ëª¨ë¸ êµ¬ì¡°"""
    def __init__(self, num_classes=17):
        super().__init__()
        resnet = models.resnet50(weights=None)
        self.features = nn.Sequential(*(list(resnet.children())[:-2]))
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        
        common_fc = lambda out: nn.Sequential(
            nn.Linear(2048, 512), nn.ReLU(), nn.Dropout(0.5), nn.Linear(512, out)
        )
        self.cls_head = common_fc(num_classes)
        self.res_head = common_fc(1)

    def forward(self, x):
        x = torch.flatten(self.avgpool(self.features(x)), 1)
        return self.cls_head(x), self.res_head(x)

def load_all_models():
    """ë¶„ë¥˜ ë° Rz ëª¨ë¸ì„ ê°ê° ë¡œë“œí•˜ì—¬ ë°˜í™˜"""
    try:
        # ë¶„ë¥˜ ëª¨ë¸ (3 Classes)
        cls_m = models.resnet50(weights=None)
        cls_m.fc = nn.Linear(cls_m.fc.in_features, 3)
        cls_m.load_state_dict(torch.load(MODEL_CLS_PATH, map_location=DEVICE))
        
        # Rz ì¶”ë¡  ëª¨ë¸ (17 Classes)
        rz_m = ResNetMultiTask(num_classes=17)
        rz_m.load_state_dict(torch.load(MODEL_RZ_PATH, map_location=DEVICE))
        
        for m in [cls_m, rz_m]: m.to(DEVICE).eval()
        logger.info("âœ… ëª¨ë“  AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        return cls_m, rz_m
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None

# ì „ì²˜ë¦¬ ì„¤ì •
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# 

def get_vision_rz(frame):
    """HSV ë§ˆìŠ¤í‚¹ ê¸°ë°˜ ê°ë„ ë° ì¤‘ì‹¬ì  ê³„ì‚°"""
    roi = frame[70:330, 90:390]
    hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)
    mask = cv2.inRange(hsv, np.array([0, 0, 210]), np.array([180, 255, 255]))
    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((10,10), np.uint8))
    
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not contours: return None, None, 0
    
    rect = cv2.minAreaRect(max(contours, key=cv2.contourArea))
    (cx, cy), (w, h), angle = rect
    final_rz = -angle + 90 if w < h else -angle
    return np.clip(final_rz, -90, 90), (cx + 90, cy + 70), cv2.contourArea(max(contours, key=cv2.contourArea))

async def send_img_result(module_type, confidence, pick_coord, status, image=None):
    """ê²°ê³¼ ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ë³€í™˜í•˜ì—¬ OPC UAë¡œ ì „ì†¡"""
    img_b64 = ""
    if image is not None and status != "arm_mission_failure":
        _, buffer = cv2.imencode('.jpg', cv2.resize(image, (224, 224)), [cv2.IMWRITE_JPEG_QUALITY, 80])
        img_b64 = base64.b64encode(buffer).decode('utf-8')

    payload = {
        "module_type": module_type,
        "classification_confidence": confidence,
        "pick_coord": [f"{c:.2f}" for c in pick_coord],
        "img": img_b64,
        "status": status
    }

    try:
        async with AsyncuaClient(OPCUA_SERVER_URL) as client:
            obj = client.get_node(WRITE_OBJ_NODE)
            method = client.get_node(WRITE_METHOD_NODE)
            await obj.call_method(method.nodeid, ua.Variant(json.dumps(payload), ua.VariantType.String))
            logger.info(f"ğŸ“¡ OPC UA ê²°ê³¼ ì†¡ì‹ : {status}")
    except Exception as e:
        logger.error(f"ğŸ“¡ ì†¡ì‹  ì˜¤ë¥˜: {e}")

async def send_single_result():
    payload = {
        "status": "arm_place_single"
    }

    try:
        async with AsyncuaClient(OPCUA_SERVER_URL) as client:
            obj = client.get_node(WRITE_SINGLE_OBJ_NODE)
            method = client.get_node(WRITE_SINGLE_METHOD_NODE)
            await obj.call_method(method.nodeid, ua.Variant(json.dumps(payload), ua.VariantType.String))
            logger.info(f"ğŸ“¡ OPC UA ê²°ê³¼ ì†¡ì‹ : {payload}")
    except Exception as e:
        logger.error(f"ğŸ“¡ ì†¡ì‹  ì˜¤ë¥˜: {e}")

async def send_completed_result():
    payload = {
        "status": "arm_place_completed"
    }

    try:
        async with AsyncuaClient(OPCUA_SERVER_URL) as client:
            obj = client.get_node(WRITE_COMPLETE_OBJ_NODE)
            method = client.get_node(WRITE_COMPLETE_METHOD_NODE)
            await obj.call_method(method.nodeid, ua.Variant(json.dumps(payload), ua.VariantType.String))
            logger.info(f"ğŸ“¡ OPC UA ê²°ê³¼ ì†¡ì‹ : {payload}")
    except Exception as e:
        logger.error(f"ğŸ“¡ ì†¡ì‹  ì˜¤ë¥˜: {e}")

#

def find_red_center(frame):
    """ ì£¼ì–´ì§„ ì´ë¯¸ì§€ í”„ë ˆì„ì—ì„œ ê°€ì¥ í° ë¹¨ê°„ìƒ‰ ì˜ì—­ì˜ ì¤‘ì‹¬ í”½ì…€ (u, v)ë¥¼ ì°¾ê³  ìœ¤ê³½ì„ ì„ ë°˜í™˜í•©ë‹ˆë‹¤. """
    
    hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    
    # ë‘ ê°œì˜ ë¹¨ê°„ìƒ‰ ë²”ìœ„ ë§ˆìŠ¤í¬ë¥¼ í•©ì¹˜ê¸° (0~10ë„, 160~179ë„)
    mask1 = cv2.inRange(hsv_frame, LOWER_RED_HSV1, UPPER_RED_HSV1)
    mask2 = cv2.inRange(hsv_frame, LOWER_RED_HSV2, UPPER_RED_HSV2)
    red_mask = cv2.bitwise_or(mask1, mask2)
    
    # ìœ¤ê³½ì„  ì°¾ê¸°
    contours, _ = cv2.findContours(red_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    if contours:
        # ê°€ì¥ í° ìœ¤ê³½ì„  ì„ íƒ
        largest_contour = max(contours, key=cv2.contourArea)
        
        if cv2.contourArea(largest_contour) > 50: # ìµœì†Œ ë©´ì  í•„í„°ë§
            M = cv2.moments(largest_contour)
            if M["m00"] != 0:
                center_x = int(M["m10"] / M["m00"])
                center_y = int(M["m01"] / M["m00"])
                return (center_x, center_y, largest_contour)
                
    return (None, None, None) # ê²€ì¶œ ì‹¤íŒ¨ ì‹œ None ë°˜í™˜

def convert_pixel_to_robot_move(current_center_u, current_center_v):
    global TARGET_CENTER_U, TARGET_CENTER_V, PIXEL_TO_MM_X, PIXEL_TO_MM_Y
    
    delta_u_pixel = current_center_u - TARGET_CENTER_U
    delta_v_pixel = current_center_v - TARGET_CENTER_V
    
    delta_X_mm = delta_u_pixel * PIXEL_TO_MM_X
    delta_Y_mm = delta_v_pixel * PIXEL_TO_MM_Y
    
    final_delta_X = -delta_X_mm
    final_delta_Y = -delta_Y_mm
    
    return final_delta_X, final_delta_Y, delta_u_pixel, delta_v_pixel

#

class SubHandler:
    def __init__(self, mc, cap, cls_m, rz_m):
        self.mc, self.cap = mc, cap
        self.cls_m, self.rz_m = cls_m, rz_m
        self.db = DBManager() # DB ë§¤ë‹ˆì € ì´ˆê¸°í™”
        self.current_mission_id = None

    def datachange_notification(self, node, val, data):
        asyncio.create_task(self.process_command(val))

    async def wait_stop(self, delay=2.0):
        while await asyncio.to_thread(self.mc.is_moving):
            await asyncio.sleep(0.2)
        await asyncio.sleep(delay)

    async def process_command(self, val):
        try:
            try:
                cmd_data = json.loads(val)
                cmd = cmd_data.get("move_command")
            except:
                cmd = str(val)

            # 2. ë¬´ì˜ë¯¸í•œ í˜¸ì¶œ í•„í„°ë§ (ëª…ë ¹ì´ ìˆì„ ë•Œë§Œ DB ì‹œì‘)
            if cmd not in ["go_home", "mission_start"]:
                return
            
            if self.current_mission_id is None:
                self.current_mission_id = await self.db.insert_mission_start()
                logger.info(f"ğŸ†• ë¯¸ì…˜ ID ìë™ ìƒì„± (ID: {self.current_mission_id})")

            logger.info(f"ğŸ“¥ ìˆ˜ì‹  ëª…ë ¹: {cmd} (Mission ID: {self.current_mission_id})")

            # 4. ëª…ë ¹ ì‹¤í–‰
            if cmd == "go_home":
                await self.move_home()
            elif cmd == "mission_start":
                if EXECUTE_MISSION_COUNT > 0 and EXECUTE_MISSION_COUNT % LOAD_OBJECT_COUNT == 0:
                    self.current_mission_id = await self.db.insert_mission_start()
                    logger.info(f"ğŸ†• ìƒˆë¡œìš´ ë¯¸ì…˜ ì„¸ì…˜ ì‹œì‘ (ID: {self.current_mission_id})")
                
                await self.execute_mission()

        except Exception as e:
            logger.error(f"âŒ ëª…ë ¹ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            # [ìˆ˜ì •] current_mission_idê°€ í™•ì‹¤íˆ ìˆì„ ë•Œë§Œ ë¡œê·¸ ì‹œë„
            if self.current_mission_id is not None:
                try:
                    await self.db.insert_arm_log(self.current_mission_id, 'ERROR', result_status='FAIL', result_message=str(e))
                    await self.db.update_mission_status(self.current_mission_id, 'ERROR')
                except:
                    logger.error("DB ë¡œê·¸ ê¸°ë¡ë§ˆì € ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                    
    async def move_home(self):
        self.mc.send_coords(INTERMEDIATE_POSE, MOVEMENT_SPEED)
        await self.db.insert_arm_log(self.current_mission_id, 'MOVE', target_pose=INTERMEDIATE_POSE, result_status='SUCCESS', description="ì„ì‹œ Conveyor ìº¡ì²˜ í¬ì¦ˆë¡œ ì´ë™")
        await self.wait_stop()
        self.mc.send_angles(CONVEYOR_CAPTURE_POSE, MOVEMENT_SPEED)
        await self.db.insert_arm_log(self.current_mission_id, 'HOME', target_pose=CONVEYOR_CAPTURE_POSE, result_status='SUCCESS', description="Conveyor ìº¡ì²˜ í¬ì¦ˆë¡œ ì´ë™")
        await self.wait_stop()

    async def execute_mission(self):
        global EXECUTE_MISSION_COUNT
        EXECUTE_MISSION_COUNT += 1
        
        for _ in range(10):
            self.cap.grab() # grab()ì€ ì´ë¯¸ì§€ ë””ì½”ë”©ì„ ì•ˆ í•´ì„œ read()ë³´ë‹¤ í›¨ì”¬ ë¹ ë¦…ë‹ˆë‹¤.
        ret, frame = self.cap.read()

        # 1. Capture & AI Inference
        ret, frame = self.cap.read()
        if not ret: return
        
        # AI & Vision Ensemble (Pick Angle)
        input_t = transform(Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))).unsqueeze(0).to(DEVICE)
        with torch.no_grad():
            cls_out = self.cls_m(input_t)
            conf, idx = torch.max(torch.softmax(cls_out, 1), 1)
            _, res_out = self.rz_m(input_t)
            ai_rz = np.clip(RZ_CENTERS[idx.item()] + res_out.item(), -90, 90)
        
        vis_rz, _, area = get_vision_rz(frame)
        final_rz = (0.8 * vis_rz + 0.2 * ai_rz) if vis_rz is not None and area > 500 else ai_rz
        
        # 2. Pick Action
        pick_pose = list(BASE_PICK_COORDS)
        pick_pose[5] = final_rz

        await send_img_result(
            module_type=CLASS_NAMES[idx.item()], 
            confidence=conf.item(), 
            pick_coord=pick_pose, 
            status="ì´ë¯¸ì§€ ì „ì†¡ ì™„ë£Œ -> ë™ì‘ ì‹œì‘", 
            image=frame)
        
        # ë™ì‘ ì‹œí€€ìŠ¤ (Safety -> Pick -> Close)
        for z_off in [50, 0]:
            p = list(pick_pose); p[2] += z_off
            self.mc.send_coords(p, MOVEMENT_SPEED - 20)
            await self.wait_stop()
        await self.db.insert_arm_log(self.current_mission_id, 'MOVE', target_pose=BASE_PICK_COORDS, result_status='SUCCESS', module_type=CLASS_NAMES[idx.item()], description="Pick í¬ì¦ˆë¡œ ì´ë™")
        
        self.mc.set_gripper_value(GRIPPER_CLOSE, GRIPPER_SPEED)
        await asyncio.sleep(GRIPPER_DELAY)
        await self.db.insert_arm_log(self.current_mission_id, 'GRIPPER_CLOSE', target_pose=GRIPPER_CLOSE, result_status='SUCCESS', module_type=CLASS_NAMES[idx.item()], description="ê·¸ë¦¬í¼ ë‹«ê¸° ì™„ë£Œ")
        
        await self.db.insert_arm_log(self.current_mission_id, 'PICK', target_pose=pick_pose, result_status='SUCCESS', module_type=CLASS_NAMES[idx.item()], description="Pick ì™„ë£Œ")
#__________________________End pick process__________________________

        # 3. Place Action (Vision-Guided)
        self.mc.send_angles(ROBOTARM_CAPTURE_POSE, MOVEMENT_SPEED)
        await self.db.insert_arm_log(self.current_mission_id, 'MOVE', target_pose=ROBOTARM_CAPTURE_POSE, result_status='SUCCESS', module_type=CLASS_NAMES[idx.item()], description="ë¡œë´‡ ì•” ìº¡ì²˜ í¬ì¦ˆë¡œ ì´ë™")
        await self.wait_stop()
        
        # ì¹´ë©”ë¼ ì”ìƒ ì œê±°ë¥¼ ìœ„í•œ ë²„í¼ ë¹„ìš°ê¸°
        for _ in range(15):
            self.cap.read()
            await asyncio.sleep(0.01)

        # í˜„ì¬ í”„ë ˆì„ ìº¡ì²˜ ë° ë¹¨ê°„ìƒ‰ ì¤‘ì‹¬ì  ì°¾ê¸°
        ret, frame = self.cap.read()
        if not ret:
            logger.error("âŒ Placeìš© í”„ë ˆì„ ìˆ˜ì‹  ì‹¤íŒ¨")
            return

        # ë¹¨ê°„ìƒ‰ ì˜ì—­ ê²€ì¶œ (ê¸°ì¡´ find_red_center í•¨ìˆ˜ í˜¸ì¶œ)
        center_u, center_v, _ = find_red_center(frame)
        
        if center_u is None:
            logger.error("ğŸ”´ ë¹¨ê°„ìƒ‰ ë¬¼ì²´ ë¯¸ê²€ì¶œ. Place ë™ì‘ì„ ì¤‘ë‹¨í•˜ê³  ì•ˆì „ ìœ„ì¹˜ë¡œ ë³µê·€í•©ë‹ˆë‹¤.")
            self.mc.send_angles(ROBOTARM_CAPTURE_POSE, MOVEMENT_SPEED)
            await self.db.insert_arm_log(self.current_mission_id, 'ERROR', target_pose=ROBOTARM_CAPTURE_POSE, result_status='SUCCESS', module_type=CLASS_NAMES[idx.item()], description="[ë¯¸ê²€ì¶œ] ì•ˆì „ í¬ì¦ˆë¡œ ì´ë™")
            await self.wait_stop()
            return

        # í”½ì…€ ì˜¤ì°¨ -> ë¡œë´‡ ì´ë™ëŸ‰(mm) ë³€í™˜
        delta_X_mm, delta_Y_mm, _, _ = convert_pixel_to_robot_move(center_u, center_v)
        
        # ìµœì¢… ëª©í‘œ ì¢Œí‘œ ìƒì„± (ê¸°ì¤€ ì¢Œí‘œ + ë³´ì •ê°’)
        final_place_coords = list(GLOBAL_TARGET_COORDS)
        final_place_coords[0] += delta_X_mm
        final_place_coords[1] += delta_Y_mm
        final_place_coords[2] = PICK_Z_HEIGHT  # ë‚´ë ¤ë†“ì„ ë†’ì´

        logger.info(f"âœ… Place ëª©í‘œ í™•ì •: X:{final_place_coords[0]:.2f}, Y:{final_place_coords[1]:.2f}")

        # ---------------------------------------------------------
        # 6. Place ë™ì‘ ì‹¤í–‰ (ì´ë™ ë° ê·¸ë¦¬í¼ ì œì–´)
        # ---------------------------------------------------------
        
        # ì•ˆì „ ì´ë™ì„ ìœ„í•œ ì„ì‹œ í¬ì¦ˆ (Zì¶• ë†’ì€ ê³³)
        safe_place_tmp = list(GLOBAL_TARGET_TMP_COORDS)

        # [STEP 1] Place êµ¬ì—­ ìœ„ ì•ˆì „ í¬ì¦ˆë¡œ ì´ë™
        logger.info("â¬†ï¸ Place ì•ˆì „ í¬ì¦ˆë¡œ ì´ë™ ì¤‘...")
        self.mc.send_coords(safe_place_tmp, MOVEMENT_SPEED - 20)
        await self.db.insert_arm_log(self.current_mission_id, 'MOVE', target_pose=safe_place_tmp, result_status='SUCCESS', module_type=CLASS_NAMES[idx.item()], description="[Place] ì•ˆì „ í¬ì¦ˆë¡œ ì´ë™")
        await self.wait_stop()

        # [STEP 2] ê³„ì‚°ëœ ì •ë°€ ì¢Œí‘œë¡œ í•˜ê°•
        logger.info("â¬‡ï¸ ì •ë°€ Place ì§€ì ìœ¼ë¡œ í•˜ê°• ì¤‘...")
        self.mc.send_coords(final_place_coords, MOVEMENT_SPEED - 30)
        await self.db.insert_arm_log(self.current_mission_id, 'MOVE', target_pose=final_place_coords, result_status='SUCCESS', module_type=CLASS_NAMES[idx.item()], description="Place ì‘ì—… ì‹œì‘")
        await self.wait_stop()

        # [STEP 3] ê·¸ë¦¬í¼ ì—´ê¸° (ë‚´ë ¤ë†“ê¸°)
        logger.info("âœŠ ê·¸ë¦¬í¼ ê°œë°© (Place ì™„ë£Œ)")
        self.mc.set_gripper_value(GRIPPER_OPEN, GRIPPER_SPEED)
        await self.db.insert_arm_log(self.current_mission_id, 'GRIPPER_OPEN', target_pose=GRIPPER_OPEN, result_status='SUCCESS', module_type=CLASS_NAMES[idx.item()], description="ê·¸ë¦¬í¼ ì—´ê¸° ì™„ë£Œ")
        await self.wait_stop()

        await self.db.insert_arm_log(self.current_mission_id, 'PLACE', target_pose=final_place_coords, result_status='SUCCESS', module_type=CLASS_NAMES[idx.item()], description="Place ì™„ë£Œ")

        # [STEP 4] ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•´ ë‹¤ì‹œ ìœ„ë¡œ ë³µê·€
        logger.info("â¬†ï¸ ë³µê·€: ë‹¤ì‹œ ì•ˆì „ í¬ì¦ˆë¡œ ì´ë™")
        self.mc.send_coords(safe_place_tmp, MOVEMENT_SPEED)
        await self.db.insert_arm_log(self.current_mission_id, 'MOVE', target_pose=safe_place_tmp, result_status='SUCCESS', module_type=CLASS_NAMES[idx.item()], description="[Place] ì™„ë£Œ ì•ˆì „ í¬ì¦ˆë¡œ ì´ë™")
        await self.wait_stop()

        logger.info("ğŸ ëª¨ë“  ë¯¸ì…˜ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

        if EXECUTE_MISSION_COUNT % LOAD_OBJECT_COUNT == 0:
            await send_completed_result()
            logger.info("ğŸ“¡ OPC UA ì „ì†¡: send_completed_result")
            await self.db.update_mission_status(self.current_mission_id, 'DONE')
            logger.info(f"âœ… ë¯¸ì…˜ ì™„ë£Œ ê¸°ë¡ (ID: {self.current_mission_id})")
        else:
            await send_single_result()
            logger.info("ğŸ“¡ OPC UA ì „ì†¡: send_single_result")
# 

async def main():
    cls_m, rz_m = load_all_models()
    if not cls_m: return

    try:
        mc = MyCobot320(PORT, BAUD)
        mc.power_on()
        print(f"\nğŸ¤– MyCobot ì—°ê²° ì„±ê³µ: {PORT}. ì´ˆê¸° ìƒíƒœ: íŒŒì›Œ ON (ì„œë³´ ì ê¸ˆ)")

        # ê·¸ë¦¬í¼ ì´ˆê¸°í™” ë¡œì§
        mc.set_gripper_mode(0)
        mc.init_electric_gripper()
        time.sleep(2)
        mc.set_electric_gripper(0)
        mc.set_gripper_value(GRIPPER_OPEN, GRIPPER_SPEED, 1) # GRIPPER_OPEN_VALUE (85)ë¡œ ì—´ë¦¼
        time.sleep(2)
        print(f"âœ… ê·¸ë¦¬í¼ ì´ˆê¸°í™” ì™„ë£Œ. ìœ„ì¹˜: **{GRIPPER_OPEN} (ì—´ë¦¼)**.")

        cap = cv2.VideoCapture(CAMERA_INDEX, cv2.CAP_DSHOW)
        cap.set(cv2.CAP_PROP_AUTOFOCUS, 1)
        
        async with AsyncuaClient(OPCUA_SERVER_URL) as client:
            handler = SubHandler(mc, cap, cls_m, rz_m)
            sub = await client.create_subscription(100, handler)
            await sub.subscribe_data_change(client.get_node(READ_METHOD_NODE))
            
            logger.info("ğŸš€ ì‹œìŠ¤í…œ ê°€ë™ ì¤‘... ëª…ë ¹ ëŒ€ê¸°")
            while True: await asyncio.sleep(1)
            
    except Exception as e:
        logger.error(f"ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
    finally:
        if 'mc' in locals(): mc.close()
        if 'cap' in locals(): cap.release()

if __name__ == "__main__":
    asyncio.run(main())