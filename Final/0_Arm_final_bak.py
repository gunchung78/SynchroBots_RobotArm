import torch
import cv2
import time
import os
import sys
import numpy as np
import json
import asyncio
from asyncua import ua
from asyncua.client import Client as AsyncuaClient
from pymycobot import MyCobot320
from torchvision import transforms
from torchvision import models
import torch.nn as nn
from PIL import Image
import base64
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ===============================================
# ğŸ“Œ AI ëª¨ë¸ ë° ìƒìˆ˜ ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼)
# ===============================================
CLASS_NAMES = ["ESP32", "L298N(Motor)", "MB102(Power)"]
NUM_CLASSES = len(CLASS_NAMES)
# ResNetìš© ì •ê·œí™” ì„¤ì • (ImageNet ê¸°ì¤€)
RESNET_MEAN = [0.485, 0.456, 0.406]
RESNET_STD = [0.229, 0.224, 0.225]

# ë¶„ë¥˜ ëª¨ë¸ê³¼ Rz ì¶”ë¡  ëª¨ë¸ ê²½ë¡œ
MODEL_CLS_PATH = "best_trck_obj_cls_model.pth"
MODEL_RZ_PATH = "best_trck_coords_tracking_model.pth"
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

RZ_CENTERS = np.arange(-90 + 5, 70 + 5 + 1e-6, 10, dtype=np.float32)

# âš™ï¸ MyCobot ë° ë¹„ì „ ì‹œìŠ¤í…œ ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼)
PORT = "COM3"
BAUD = 115200
MOVEMENT_SPEED = 70
PICK_Z_HEIGHT = 260
GRIPPER_SPEED = 50
GRIPPER_OPEN_VALUE = 85
GRIPPER_CLOSED_VALUE = 25
SEQUENTIAL_MOVE_DELAY = 3
GRIPPER_ACTION_DELAY = 1

CAMERA_INDEX = 0
roi_start = (0, 0)
roi_end = (640, 360)
TARGET_CENTER_U = 320
TARGET_CENTER_V = 180

PIXEL_TO_MM_X = 0.526
PIXEL_TO_MM_Y = -0.698

MAX_PIXEL_ERROR = 5

LOWER_HSV = np.array([0, 0, 210])
UPPER_HSV = np.array([180, 255, 255])

CONVEYOR_CAPTURE_POSE = [0, 0, 90, 0, -90, -90]
ROBOTARM_CAPTURE_POSE = [0, 0, 10, 80, -90, 90]

INTERMEDIATE_POSE_ANGLES = [-17.2, 30.49, 4.48, 53.08, -90.87, -85.86]
ZERO_POSE_ANGLES = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

BASE_PICK_COORDS = [-237.90, 20, 183.6, -174.98, 0, 0]

# --- Place ë³€ìˆ˜
LOWER_RED_HSV1 = np.array([0, 100, 100])
UPPER_RED_HSV1 = np.array([15, 255, 255])
LOWER_RED_HSV2 = np.array([155, 100, 100])
UPPER_RED_HSV2 = np.array([179, 255, 255])

GLOBAL_TARGET_COORDS = [-114, -195, 250, 177.71, 0.22, 0]
GLOBAL_TARGET_TMP_COORDS = [-150.0, -224.4, 318.1, 176.26, 3.2, 3.02]

# --- ğŸ¯ OPC UA ìˆ˜ì‹ /ì†¡ì‹  ì„¤ì • (ìˆ˜ì •ëœ ë¶€ë¶„) ---
OPCUA_READ_URL = "opc.tcp://172.30.1.61:0630/freeopcua/server/"
OPCUA_WRITE_URL = "opc.tcp://172.30.1.61:0630/freeopcua/server/"

# ğŸ“Œ ì½ê¸°(êµ¬ë…) ë…¸ë“œ ID
READ_OBJECT_NODE_ID = "ns=2;i=3"
READ_METHOD_NODE_ID = "ns=2;s=read_arm_go_move"

# ğŸ“Œ ì“°ê¸°(Method Call) ë…¸ë“œ ID
WRITE_OBJECT_NODE_ID = "ns=2;i=3"
WRITE_METHOD_NODE_ID = "ns=2;s=write_send_arm_json"

# ì „ì—­ ê°ì²´ (ë©”ì¸ ë° í•¸ë“¤ëŸ¬ì—ì„œ ì‚¬ìš©)
mc = None
cap = None
ai_model = None
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=RESNET_MEAN, std=RESNET_STD)
])

# ===============================================
# ğŸ§  AI ëª¨ë¸ ì •ì˜ ë° ë¡œë“œ í•¨ìˆ˜
# ===============================================

class ResNetMultiTask(nn.Module):
    def __init__(self, num_classes=17): # í•™ìŠµ ì‹œ 17ê°œ í´ë˜ìŠ¤ì˜€ìœ¼ë¯€ë¡œ ê¸°ë³¸ê°’ 17
        super(ResNetMultiTask, self).__init__()
        resnet = models.resnet50(weights=None)
        
        self.features = nn.Sequential(*(list(resnet.children())[:-2]))
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        
        self.cls_head = nn.Sequential(
            nn.Linear(2048, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, num_classes)
        )
        
        self.res_head = nn.Sequential(
            nn.Linear(2048, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, 1)
        )

    def forward(self, x):
        x = self.features(x)
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        cls_output = self.cls_head(x)
        res_output = self.res_head(x)
        return cls_output, res_output

def load_all_models():
    """ë‘ ê°œì˜ ëª¨ë¸(ë¶„ë¥˜, Rz ì¶”ë¡ )ì„ ê°ê°ì˜ êµ¬ì¡°ì— ë§ì¶° ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        # --- [1] ê°ì²´ ë¶„ë¥˜ ëª¨ë¸ (best_trck_obj_cls_model.pth) ---
        # í•™ìŠµ ì½”ë“œ ê¸°ì¤€: ResNet50ì˜ ë§ˆì§€ë§‰ fcë§Œ ìˆ˜ì •ëœ í˜•íƒœ (í´ë˜ìŠ¤ 3ê°œ)
        cls_model = models.resnet50(weights=None)
        num_ftrs = cls_model.fc.in_features
        cls_model.fc = nn.Linear(num_ftrs, 3) # ESP32, L298N, MB102
        cls_model.load_state_dict(torch.load(MODEL_CLS_PATH, map_location=DEVICE))
        cls_model.to(DEVICE).eval()
        logger.info("âœ… ë¶„ë¥˜ ëª¨ë¸(Classification) ë¡œë“œ ì™„ë£Œ (Output: 3)")

        # --- [2] Rz ì¶”ë¡  ëª¨ë¸ (best_trck_coords_tracking_model.pth) ---
        # í•™ìŠµ ì½”ë“œ ê¸°ì¤€: ResNetMultiTask êµ¬ì¡° (í´ë˜ìŠ¤ 17ê°œ)
        # ì—ëŸ¬ ë©”ì‹œì§€ì—ì„œ shape torch.Size([17, 512])ë¼ê³  í–ˆìœ¼ë¯€ë¡œ 17ë¡œ ì„¤ì •í•´ì•¼ í•¨
        rz_model = ResNetMultiTask(num_classes=17) 
        rz_model.load_state_dict(torch.load(MODEL_RZ_PATH, map_location=DEVICE))
        rz_model.to(DEVICE).eval()
        logger.info("âœ… Rz ì¶”ë¡  ëª¨ë¸(MultiTask) ë¡œë“œ ì™„ë£Œ (Output: 17)")

        return cls_model, rz_model
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None

# ì „ì—­ ëª¨ë¸ ë³€ìˆ˜
cls_model, rz_model = load_all_models()
test_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=RESNET_MEAN, std=RESNET_STD)
])

def classify_object(model, transform, cropped_img):
    """í¬ë¡­ëœ ì´ë¯¸ì§€ë¡œ ê°ì²´ ë¶„ë¥˜ ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    if cropped_img is None or cropped_img.size == 0:
        return "Unknown", 0.0

    # OpenCV (BGR) -> RGB
    rgb_frame = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB)
    
    # NumPy ë°°ì—´ -> PIL Image -> Tensorë¡œ ë³€í™˜ ë° ì •ê·œí™”
    pil_image = Image.fromarray(rgb_frame)
    input_tensor = transform(pil_image).unsqueeze(0).to(DEVICE)
    
    with torch.no_grad():
        outputs = model(input_tensor)
        probabilities = torch.nn.functional.softmax(outputs, dim=1)
        conf_score, predicted_idx = torch.max(probabilities, 1)
        
        predicted_class = CLASS_NAMES[predicted_idx.item()]
        confidence = conf_score.item()
        
    return predicted_class, confidence

def get_vision_rz(img):
    """HSV ë§ˆìŠ¤í‚¹ ë° minAreaRectë¥¼ í†µí•œ Vision ê¸°ë°˜ Rz ê³„ì‚°"""
    x_start, y_start, x_end, y_end = 90, 70, 390, 330
    roi = img[y_start:y_end, x_start:x_end]
    
    hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)
    # ê¸°íŒ ì¶”ì¶œì„ ìœ„í•œ ì„ê³„ê°’ (ì‚¬ìš©ì ì½”ë“œ ì°¸ì¡°)
    lower_bound = np.array([0, 0, 210]) 
    upper_bound = np.array([180, 255, 255])
    
    mask = cv2.inRange(hsv, lower_bound, upper_bound)
    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((10,10), np.uint8))
    
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not contours:
        return None, None, 0
    
    main_contour = max(contours, key=cv2.contourArea)
    rect = cv2.minAreaRect(main_contour)
    (cx, cy), (w, h), angle = rect
    
    # ê°ë„ ë³´ì • ë¡œì§
    if w < h:
        angle += 90
    vision_rz = -angle + 90
    vision_rz = np.clip(vision_rz, -90, 90)
    
    return vision_rz, (cx + x_start, cy + y_start), cv2.contourArea(main_contour)

def ensemble_rz(rz_vision, rz_ai, area):
    """Visionê³¼ AIì˜ ê²°ê³¼ë¥¼ ê²°í•©"""
    if rz_vision is None: return rz_ai
    
    THRESHOLD = 500
    w_vis = 0.8 if area >= THRESHOLD else 0.4
    w_ai = 1.0 - w_vis
    
    final_rz = (w_vis * rz_vision) + (w_ai * rz_ai)
    return np.clip(final_rz, -90, 90)

# ===============================================
# ğŸš€ ìˆ˜ì •ëœ pick_data_collector
# ===============================================

async def pick_data_collector(cap, cls_model, rz_model):
    """ë¶„ë¥˜, Rz ì¶”ë¡ , Vision ì•™ìƒë¸”ì„ í†µí•´ ìµœì¢… í”½ì—… íŒŒë¼ë¯¸í„°ë¥¼ ê²°ì •"""
    global test_transform, RZ_CENTERS
    
    ret, frame = cap.read()
    if not ret: return False, "Unknown", 0.0, [0.0]*6, None

    # 1. AI ì¶”ë¡  (ë¶„ë¥˜ ë° Rz ì”ì°¨)
    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    img_pil = Image.fromarray(img_rgb)
    input_tensor = test_transform(img_pil).unsqueeze(0).to(DEVICE)

    with torch.no_grad():
        # ë¶„ë¥˜ ëª¨ë¸ë¡œ í´ë˜ìŠ¤ í™•ì¸
        cls_out = cls_model(input_tensor)
        prob = torch.nn.functional.softmax(cls_out, dim=1)
        conf, cls_idx = torch.max(prob, 1)
        predicted_class = CLASS_NAMES[cls_idx.item()]
        
        # Rz ëª¨ë¸ë¡œ ê°ë„ ì¶”ë¡ 
        _, res_out = rz_model(input_tensor)
        predicted_residual = res_out.squeeze().item()
        ai_rz = np.clip(RZ_CENTERS[cls_idx.item()] + predicted_residual, -90, 90)

    # 2. Vision ì¶”ë¡ 
    vision_rz, center_pt, area = get_vision_rz(frame.copy())

    # 3. ì•™ìƒë¸” ê²°ì •
    final_rz = ensemble_rz(vision_rz, ai_rz, area)
    
    logger.info(f"Final Decision -> Class: {predicted_class}, Rz: {final_rz:.2f}Â° (AI: {ai_rz:.2f}, Vis: {vision_rz})")

    # 4. ìµœì¢… ë¡œë´‡ íƒ€ê²Ÿ í¬ì¦ˆ ìƒì„± (J6 ê°ë„ì— final_rz ë°˜ì˜)
    # ê¸°ì¡´ TEST_PICK_POSE_WIDTH ë“±ì„ ê¸°ë°˜ìœ¼ë¡œ ë§ˆì§€ë§‰ ì¸ë±ìŠ¤(Rz) ìˆ˜ì •
    target_pose = list(BASE_PICK_COORDS)
    target_pose[5] = final_rz 
    
    # ì„œë²„ ì†¡ì‹ ìš© ì´ë¯¸ì§€ crop
    send_img = frame[30:400, 30:340]
    
    return True, predicted_class, conf.item(), target_pose, send_img

def convert_pixel_to_robot_move(current_center_u, current_center_v):
    global TARGET_CENTER_U, TARGET_CENTER_V, PIXEL_TO_MM_X, PIXEL_TO_MM_Y
    
    delta_u_pixel = current_center_u - TARGET_CENTER_U
    delta_v_pixel = current_center_v - TARGET_CENTER_V
    
    delta_X_mm = delta_u_pixel * PIXEL_TO_MM_X
    delta_Y_mm = delta_v_pixel * PIXEL_TO_MM_Y
    
    final_delta_X = -delta_X_mm
    final_delta_Y = -delta_Y_mm
    
    return final_delta_X, final_delta_Y, delta_u_pixel, delta_v_pixel

def find_red_center(frame):
    """ ì£¼ì–´ì§„ ì´ë¯¸ì§€ í”„ë ˆì„ì—ì„œ ê°€ì¥ í° ë¹¨ê°„ìƒ‰ ì˜ì—­ì˜ ì¤‘ì‹¬ í”½ì…€ (u, v)ë¥¼ ì°¾ê³  ìœ¤ê³½ì„ ì„ ë°˜í™˜í•©ë‹ˆë‹¤. """
    
    hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    
    # ë‘ ê°œì˜ ë¹¨ê°„ìƒ‰ ë²”ìœ„ ë§ˆìŠ¤í¬ë¥¼ í•©ì¹˜ê¸° (0~10ë„, 160~179ë„)
    mask1 = cv2.inRange(hsv_frame, LOWER_RED_HSV1, UPPER_RED_HSV1)
    mask2 = cv2.inRange(hsv_frame, LOWER_RED_HSV2, UPPER_RED_HSV2)
    red_mask = cv2.bitwise_or(mask1, mask2)
    
    # ìœ¤ê³½ì„  ì°¾ê¸°
    contours, _ = cv2.findContours(red_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    if contours:
        # ê°€ì¥ í° ìœ¤ê³½ì„  ì„ íƒ
        largest_contour = max(contours, key=cv2.contourArea)
        
        if cv2.contourArea(largest_contour) > 50: # ìµœì†Œ ë©´ì  í•„í„°ë§
            M = cv2.moments(largest_contour)
            if M["m00"] != 0:
                center_x = int(M["m10"] / M["m00"])
                center_y = int(M["m01"] / M["m00"])
                return (center_x, center_y, largest_contour)
                
    return (None, None, None) # ê²€ì¶œ ì‹¤íŒ¨ ì‹œ None ë°˜í™˜

# ===============================================
# ğŸ¤– ë¡œë´‡ ë¹„ë™ê¸° í—¬í¼ í•¨ìˆ˜ (ì•ˆì „ ë§ˆì§„ ì¶”ê°€)
# ===============================================

async def wait_until_stopped(mc, safety_delay=2.0):
    """ ë¡œë´‡ì´ ì›€ì§ì„ì„ ì™„ì „íˆ ë©ˆì¶”ê³  ì•ˆì „ ë§ˆì§„ ì‹œê°„ë§Œí¼ ëŒ€ê¸°í•©ë‹ˆë‹¤. """
    logger.info("... ë¡œë´‡ ì›€ì§ì„ ì™„ë£Œ ëŒ€ê¸° ì¤‘ (is_moving ì²´í¬)...")

    # 1. is_movingì´ Falseë¥¼ ë°˜í™˜í•  ë•Œê¹Œì§€ ëŒ€ê¸°
    while await asyncio.to_thread(mc.is_moving):
        await asyncio.sleep(0.2)
        
    # 2. ì›€ì§ì„ì´ ë©ˆì¶˜ í›„, ë¡œë´‡ ì œì–´ê¸°ê°€ ë‹¤ìŒ ëª…ë ¹ì„ ë°›ì„ ì¤€ë¹„ê°€ ë  ì‹œê°„ì„ í™•ë³´ (ì•ˆì „ ë§ˆì§„)
    logger.info(f"... ì›€ì§ì„ ì¤‘ì§€ í™•ì¸. ì•ˆì „ ë§ˆì§„ {safety_delay}ì´ˆ ì¶”ê°€ ëŒ€ê¸°...")
    await asyncio.sleep(safety_delay) 
    
    return True

async def place_coords_calculator(cap):
    """ 
    [Vision-Guided] ê²€ì¶œëœ ë¹¨ê°„ìƒ‰ êµ¬ì—­ì˜ ì¤‘ì‹¬ì„ ê¸°ì¤€ìœ¼ë¡œ ë°°ì¹˜í•  ìµœì¢… ëª©í‘œ ì¢Œí‘œë¥¼ ê³„ì‚°í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print("â˜†â˜…â˜†â˜…â˜†â˜…â˜†â˜… place_coords_calculator: Place ëª©í‘œ ì¢Œí‘œ ê³„ì‚° ì‹œì‘")
    global GLOBAL_TARGET_COORDS, MOVEMENT_SPEED, PICK_Z_HEIGHT
    
    # 1. ì´ë¯¸ì§€ ìº¡ì²˜ ë° ì¤‘ì‹¬ ì°¾ê¸°
    ret, frame = cap.read()
    if not ret:
        print("âŒ í”„ë ˆì„ ìˆ˜ì‹  ì‹¤íŒ¨. ì¢Œí‘œ ê³„ì‚° ì¤‘ì§€.")
        return False, None
    
    DATA_DIR = "place_capture"
    filename = f"place_calc_frame.jpg"
    save_path = os.path.join(DATA_DIR, filename)
    cv2.imwrite(save_path, frame)

    print(f"ğŸ–¼ï¸ Place ê³„ì‚° í”„ë ˆì„ ì €ì¥ ì™„ë£Œ: {save_path}")
    center_u, center_v, _ = find_red_center(frame)
    
    if center_u is None:
        print(f"ğŸ”´ ë¹¨ê°„ìƒ‰ ë¬¼ì²´ ë¯¸ê²€ì¶œ. ì¢Œí‘œ ê³„ì‚° ì¤‘ì§€.")
        return False, None

    # 2. ì˜¤ì°¨ ê³„ì‚° ë° MM ë³€í™˜
    delta_X_mm, delta_Y_mm, delta_u_pixel, delta_v_pixel = convert_pixel_to_robot_move(center_u, center_v)
    
    total_pixel_error = np.sqrt(delta_u_pixel**2 + delta_v_pixel**2)
    
    print(f"\n--- ğŸ¤– Vision-Guided ì •ë ¬ ê³„ì‚° (Single Shot) ---")
    print(f"  [Detect] í”½ì…€ ì˜¤ì°¨: {total_pixel_error:.2f}px (U: {delta_u_pixel}, V: {delta_v_pixel})")
    print(f"  [Move] í•„ìš”í•œ ì´ë™ëŸ‰: X:{delta_X_mm:.2f}mm, Y:{delta_Y_mm:.2f}mm")

    # 3. ìµœì¢… ëª©í‘œ ì¢Œí‘œ ê³„ì‚°
    final_place_coords = list(GLOBAL_TARGET_COORDS) # ê¸°ì¤€ ì¢Œí‘œ ë³µì‚¬
    
    # í”½ì…€ ì˜¤ì°¨ë¥¼ MMìœ¼ë¡œ ë³€í™˜í•œ ë§Œí¼ ë¡œë´‡ ì¢Œí‘œì— ì¶”ê°€í•˜ì—¬ 'ì •ë ¬ëœ' ëª©í‘œ ì¢Œí‘œë¥¼ ìƒì„±
    final_place_coords[0] += delta_X_mm # Xì¶• ì´ë™ ëª…ë ¹ ì ìš©
    final_place_coords[1] += delta_Y_mm # Yì¶• ì´ë™ ëª…ë ¹ ì ìš©
    
    # Zì¶• ë†’ì´ëŠ” ë¯¸ë¦¬ ì„¤ì •ëœ í”½ì—… ë†’ì´ë¡œ ê³ ì •
    final_place_coords[2] = PICK_Z_HEIGHT 

    print(f"âœ… ëª©í‘œ ì¢Œí‘œ ê³„ì‚° ì™„ë£Œ. ìµœì¢… ì¢Œí‘œ: X:{final_place_coords[0]:.2f}, Y:{final_place_coords[1]:.2f}, Z:{PICK_Z_HEIGHT:.2f}")
    
    # 4. ê³„ì‚°ëœ ì¢Œí‘œ ë°˜í™˜
    return True, final_place_coords

# ===============================================
# ğŸ“¡ OPC UA í†µì‹  í•¨ìˆ˜ (ê¸°ì¡´ê³¼ ë™ì¼)
# ===============================================

async def send_full_result(module_type: str, confidence: float, pick_coord: list, status: str, image_to_send: np.ndarray = None):
    """
    ë¶„ë¥˜, í”½ì—… ê²°ê³¼ ë° ë¯¸ì…˜ ìƒíƒœë¥¼ JSON í˜•íƒœë¡œ ë¬¶ì–´ OPC UA ì„œë²„ì— í•œ ë²ˆì— ì†¡ì‹ í•©ë‹ˆë‹¤.
    """
    global OPCUA_WRITE_URL, WRITE_OBJECT_NODE_ID, WRITE_METHOD_NODE_ID
    
    # --- ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ì¸ì½”ë”© ë¡œì§ (ê¸°ì¡´ê³¼ ë™ì¼) ---
    base64_img_str = ""
    if image_to_send is not None and status != "arm_mission_failure": # ì‹¤íŒ¨ ì‹œ ì´ë¯¸ì§€ë¥¼ ë³´ë‚´ì§€ ì•Šì•„ íŠ¸ë˜í”½ ì ˆì•½
        try:
            resized_img = cv2.resize(image_to_send, (224, 224), interpolation=cv2.INTER_AREA)
            encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 80] 
            _, buffer = cv2.imencode('.jpg', resized_img, encode_param)
            base64_img_bytes = base64.b64encode(buffer)
            base64_img_str = base64_img_bytes.decode('utf-8')
            logger.info(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ì¸ì½”ë”© ì™„ë£Œ. Base64 ë¬¸ìì—´ ê¸¸ì´: {len(base64_img_str)}")
        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ ì¸ì½”ë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            base64_img_str = ""
    
    # ğŸ“Œ í†µí•©ëœ JSON ë°ì´í„° êµ¬ì„± (status í•„ë“œ ì¶”ê°€)
    vision_result = {
        "module_type": module_type,
        "classification_confidence": confidence,
        "pick_coord": [f"{c:.2f}" for c in pick_coord],
        "pick_coord_confidence": 0.9984073221683503,
        "img": base64_img_str,
        "status": status # â¬…ï¸ ë¯¸ì…˜ ìƒíƒœ í†µí•©
    }
    json_str = json.dumps(vision_result)

    # ğŸš€ í´ë¼ì´ì–¸íŠ¸ ì „ì†¡ ë°ì´í„° í™•ì¸
    print("\n========================================================")
    print(f"ğŸš€ [FULL RESULT] í´ë¼ì´ì–¸íŠ¸ê°€ ì„œë²„ë¡œ ì „ì†¡í•˜ëŠ” ìµœì¢… í†µí•© JSON:")
    print(json_str)
    print("========================================================\n")
    
    try:
        async with AsyncuaClient(OPCUA_WRITE_URL) as client:
            obj = client.get_node(WRITE_OBJECT_NODE_ID)
            method_node = client.get_node(WRITE_METHOD_NODE_ID)
            json_variant = ua.Variant(json_str, ua.VariantType.String)

            print(f"[OPC UA WRITE - FULL_RESULT] call_method(Module: {module_type}, Status: {status}) (Method: {WRITE_METHOD_NODE_ID})")
            result_code, result_message = await obj.call_method(
                method_node.nodeid,
                json_variant
            )
            logger.info(f"OPC UA í†µí•© ê²°ê³¼ ì†¡ì‹  ì™„ë£Œ. ResultCode: {result_code}")

    except Exception as e:
        logger.error(f"OPC UA í†µí•© ê²°ê³¼ ì†¡ì‹  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def sync_flush_camera_buffer(cap, num_frames=10):
        for _ in range(num_frames):
            cap.read() # í”„ë ˆì„ì„ ì½ì–´ ë²„ë¦¼
            time.sleep(0.01)

class SubHandler:
    def __init__(self, mycobot_instance, camera_instance, cls_model, rz_model):
        self.mc = mycobot_instance
        self.cap = camera_instance
        self.cls_model = cls_model
        self.rz_model = rz_model
        logger.info("SubHandler ì´ˆê¸°í™” ì™„ë£Œ (AI ëª¨ë¸ 2ê°œ ë¡œë“œë¨).")

    def datachange_notification(self, node, val, data):
        """ë°ì´í„° ë³€ê²½ ì•Œë¦¼ ì‹œ í˜¸ì¶œë˜ëŠ” ë¹„ë™ê¸°ì  ì½œë°± í•¨ìˆ˜"""
        asyncio.create_task(self.execute_command_and_respond(val))
    
    async def execute_command_and_respond(self, val):
        """
        [ìµœì¢… ìˆ˜ì •] ëª…ë ¹ì„ íŒŒì‹±í•˜ê³  MyCobot ë™ì‘ì„ ëª¨ë‘ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        vision-guided place ì´ë™ ëª…ë ¹ê¹Œì§€ ì—¬ê¸°ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        """
        global SEQUENTIAL_MOVE_DELAY, MOVEMENT_SPEED, GRIPPER_OPEN_VALUE, GRIPPER_CLOSED_VALUE, GRIPPER_SPEED, GRIPPER_ACTION_DELAY

        print(f"\nâ– â–¡â– â–¡â– â–¡â– â–¡â– â–¡â– â–¡[OPC UA READ] ìˆ˜ì‹  ê°’: {val}")

        command = None
        if isinstance(val, str):
            try:
                json_data = json.loads(val)
                if "move_command" in json_data:
                    command = json_data["move_command"]
            except json.JSONDecodeError:
                command = val # Ready ê°™ì€ ì¼ë°˜ ë¬¸ìì—´ë„ commandë¡œ ê°„ì£¼

        if not command or self.mc is None or self.cap is None:
            logger.warning(f"-> ë¡œë´‡/ì¹´ë©”ë¼ ì—°ê²° ë¬¸ì œ ë˜ëŠ” ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹: {command}")
            return
        
        # 3. MyCobot ë™ì‘ ìˆ˜í–‰ ë° ì‘ë‹µ
        if command == "go_home":
            logger.info("-> MyCobot: go_home ëª…ë ¹ ìˆ˜í–‰ (CONVEYOR_CAPTURE_POSEë¡œ ì´ë™)")
            
            # ì¤‘ê°„ í¬ì¦ˆë¡œ ì´ë™
            await asyncio.to_thread(self.mc.send_coords, INTERMEDIATE_POSE_ANGLES, MOVEMENT_SPEED)
            await wait_until_stopped(self.mc)
            
            # ì»¨ë² ì´ì–´ ìº¡ì²˜ í¬ì¦ˆë¡œ ì´ë™
            await asyncio.to_thread(self.mc.send_angles, CONVEYOR_CAPTURE_POSE, MOVEMENT_SPEED)
            await wait_until_stopped(self.mc)
            logger.info("âœ… go_home (CONVEYOR_CAPTURE_POSE) ì´ë™ ì™„ë£Œ.")
        
        elif command == "mission_start":
            logger.info("-> MyCobot: mission_start ëª…ë ¹ ìˆ˜í–‰ (Vision-Guided Pick ì‹œì‘)")
            
            # 1. Vision/AI ë°ì´í„° ìˆ˜ì§‘ (Pick ëª©í‘œ ì¢Œí‘œ)
            success, module_type, confidence, target_pick_pose, send_img = await pick_data_collector(self.cap, self.cls_model, self.rz_model)

            if not success:
                logger.error("âŒ ë°ì´í„° ìˆ˜ì§‘/ë¬¼ì²´ ê²€ì¶œ ì‹¤íŒ¨. OPC UA ì‹¤íŒ¨ ë³´ê³  ì†¡ì‹ .")
                await send_full_result(
                    module_type=module_type, confidence=confidence, 
                    pick_coord=target_pick_pose, status="arm_mission_failure"
                )
                return 

            # 2. OPC UA ê²°ê³¼ ì „ì†¡ (ë¡œë´‡ ë™ì‘ ì§ì „ì— ì •ë³´ ì „ì†¡)
            await send_full_result(
                module_type=module_type, confidence=confidence, 
                pick_coord=target_pick_pose, status="arm_mission_success", 
                image_to_send=send_img
            )

            # 3. í”½ì—… ë™ì‘ ì‹œí€€ìŠ¤ ì‹œì‘
            safe_pick_pose = list(target_pick_pose)
            safe_pick_pose[2] += 50 
            
            logger.info(f"â¬†ï¸ ì•ˆì „ í¬ì¦ˆë¡œ ì´ë™: Z:{safe_pick_pose[2]:.2f}")
            await asyncio.to_thread(self.mc.send_coords, safe_pick_pose, MOVEMENT_SPEED)
            await wait_until_stopped(self.mc)

            logger.info(f"\nâ¬‡ï¸ í”½ì—… ì‹œì‘: Z:{target_pick_pose[2]:.2f} í•˜ê°•.")
            await asyncio.to_thread(self.mc.send_coords, target_pick_pose, MOVEMENT_SPEED - 30)
            await wait_until_stopped(self.mc)
            
            await asyncio.to_thread(self.mc.set_gripper_value, GRIPPER_CLOSED_VALUE, GRIPPER_SPEED)
            await asyncio.sleep(GRIPPER_ACTION_DELAY)
            logger.info("âœ… ê·¸ë¦¬í¼ ë‹«ê¸° ì™„ë£Œ (Pick).")

            # 4. ì¤‘ê°„ í¬ì¦ˆ ì´ë™ (Place ì¤€ë¹„)
            await asyncio.to_thread(self.mc.send_angles, CONVEYOR_CAPTURE_POSE, MOVEMENT_SPEED)
            await wait_until_stopped(self.mc)
            logger.info("âœ… CONVEYOR_CAPTURE_POSE ì´ë™ ì™„ë£Œ.")

            await asyncio.to_thread(self.mc.send_angles, ROBOTARM_CAPTURE_POSE, MOVEMENT_SPEED)
            await wait_until_stopped(self.mc)
            logger.info("âœ… ROBOTARM_CAPTURE_POSE ì´ë™ ì™„ë£Œ.")
            
            await asyncio.to_thread(sync_flush_camera_buffer, self.cap, 15)
            await asyncio.sleep(0.5)

            # 5. Place ëª©í‘œ ì¢Œí‘œ ê³„ì‚° (Vision-Guided)
            print(f"\nğŸš€ Place ì‘ì—… ì‹œì‘: Vision-Guided ëª©í‘œ ì¢Œí‘œ ê³„ì‚° ì‹œì‘")
            place_calc_success, final_place_coords = await place_coords_calculator(self.cap)
            
            if not place_calc_success:
                logger.error("âŒ Place ëª©í‘œ ì¢Œí‘œ ê³„ì‚° ì‹¤íŒ¨. Place ë™ì‘ ì¤‘ë‹¨.")
                # ë¯¸ì…˜ ì‹¤íŒ¨ë¡œ ì²˜ë¦¬í•˜ì§€ ì•Šê³ , í˜„ì¬ ìœ„ì¹˜ì—ì„œ ê·¸ë¦¬í¼ë¥¼ ì—¬ëŠ” ë“± ì•ˆì „ ì¡°ì¹˜ í•„ìš”
                await asyncio.to_thread(self.mc.send_angles, ROBOTARM_CAPTURE_POSE, MOVEMENT_SPEED) # ì•ˆì „ í¬ì¦ˆë¡œ ë³µê·€
                await wait_until_stopped(self.mc)
                return 
            
            # 6. Place ë™ì‘ ì‹¤í–‰
            
            # Place ëª©í‘œì˜ ì•ˆì „ í¬ì¦ˆ (Zì¶• + 50)
            safe_place_coords = list(GLOBAL_TARGET_TMP_COORDS)
            
            # ì•ˆì „ í¬ì¦ˆë¡œ ì´ë™
            logger.info(f"â¬†ï¸ Place ì•ˆì „ í¬ì¦ˆë¡œ ì´ë™: X:{safe_place_coords[0]:.2f}, Y:{safe_place_coords[1]:.2f} (Z:{safe_place_coords[2]:.2f})")
            print(safe_place_coords)
            await asyncio.to_thread(self.mc.send_coords, safe_place_coords, MOVEMENT_SPEED - 30)
            await wait_until_stopped(self.mc)

            # Place ì§€ì ìœ¼ë¡œ í•˜ê°•
            logger.info(f"â¬‡ï¸ Place ì§€ì ìœ¼ë¡œ í•˜ê°•: X:{final_place_coords[0]:.2f}, Y:{final_place_coords[1]:.2f} (Z:{final_place_coords[2]:.2f})")
            print(final_place_coords)
            await asyncio.to_thread(self.mc.send_coords, final_place_coords, MOVEMENT_SPEED - 30)
            await wait_until_stopped(self.mc)

            print("âœŠ ê·¸ë¦¬í¼ ì—¬ëŠ” ì¤‘ (Place ë™ì‘)...")
            # ê·¸ë¦¬í¼ ì—´ê¸° (Place)
            await asyncio.to_thread(self.mc.set_gripper_value, GRIPPER_OPEN_VALUE, GRIPPER_SPEED)
            await asyncio.sleep(GRIPPER_ACTION_DELAY)
            print(f"âœ… Place ì™„ë£Œ (ê·¸ë¦¬í¼ ì—´ë¦¼).")

            # Place ì™„ë£Œ í›„ ì•ˆì „ í¬ì¦ˆë¡œ ë³µê·€
            await asyncio.to_thread(self.mc.send_coords, safe_place_coords, MOVEMENT_SPEED)
            await wait_until_stopped(self.mc)
            
        elif command == "Ready":
            logger.info("-> MyCobot: Ready ìƒíƒœ ìˆ˜ì‹ , ëŒ€ê¸° ì¤‘...")
            
        else:
            logger.warning(f"-> MyCobot: ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹: {command}")

async def arm_subscriber():
    """ OPC UA í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‹¤í–‰í•˜ê³  êµ¬ë…ì„ ì„¤ì •í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜ """
    global mc, cap, cls_model, rz_model

    # ğŸ“Œ 1. AI ëª¨ë¸ ë¡œë“œ
    cls_model, rz_model = load_all_models()
    if cls_model is None or rz_model is None:
        logger.error("AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    # ğŸ“Œ 2. MyCobot ì—°ê²° ì´ˆê¸°í™”
    try:
        mc = MyCobot320(PORT, BAUD)
        mc.set_color(0, 0, 255) 
        logger.info(f"MyCobot320ì´ {PORT}ì— {BAUD} ì†ë„ë¡œ ì„±ê³µì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ê·¸ë¦¬í¼ ì´ˆê¸°í™” ë¡œì§
        mc.set_gripper_mode(0)
        mc.init_electric_gripper()
        await asyncio.sleep(2)
        mc.set_electric_gripper(0)
        mc.set_gripper_value(GRIPPER_OPEN_VALUE, GRIPPER_SPEED, 1) # GRIPPER_OPEN_VALUE (85)ë¡œ ì—´ë¦¼
        await asyncio.sleep(2)
        logger.info(f"-> MyCobot320: ì „ê¸° ê·¸ë¦¬í¼ ì´ˆê¸°í™” ì™„ë£Œ ({GRIPPER_OPEN_VALUE} ìœ„ì¹˜ë¡œ ì´ë™).")
        
    except Exception as e:
        logger.error(f"MyCobot320 ì—°ê²° ë˜ëŠ” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        mc = None

    # ğŸ“Œ 3. ì¹´ë©”ë¼ ì—°ê²° ì´ˆê¸°í™”
    cap = cv2.VideoCapture(CAMERA_INDEX, cv2.CAP_DSHOW)
    if not cap.isOpened():
        logger.error(f"ì¹´ë©”ë¼ ì¸ë±ìŠ¤ {CAMERA_INDEX}ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        cap = None

    if mc is None or cap is None:
        logger.error("MyCobot ë˜ëŠ” ì¹´ë©”ë¼ ì—°ê²° ë¬¸ì œë¡œ Vision-Pick ë¯¸ì…˜ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        if mc is not None:
            mc.close()
        return # ì—°ê²° ì‹¤íŒ¨ ì‹œ ì¢…ë£Œ

    # ğŸ“Œ 4. OPC UA ì—°ê²° ë° êµ¬ë… ì„¤ì •
    logger.info(f"OPC UA ìˆ˜ì‹  ì„œë²„ì— ì—°ê²° ì‹œë„: {OPCUA_READ_URL}")

    try:
        async with AsyncuaClient(OPCUA_READ_URL) as client:
            logger.info("OPC UA ìˆ˜ì‹  ì„œë²„ì— ì„±ê³µì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")

            handler = SubHandler(mc, cap, cls_model, rz_model)
            # êµ¬ë… ê°„ê²© 100ms
            sub = await client.create_subscription(100, handler)

            cmd_node = client.get_node(READ_METHOD_NODE_ID) 
            await sub.subscribe_data_change(cmd_node)
            logger.info(f"ë…¸ë“œ '{READ_METHOD_NODE_ID}' êµ¬ë… ì‹œì‘. ë°ì´í„° ìˆ˜ì‹  ëŒ€ê¸° ì¤‘...")

            while True:
                await asyncio.sleep(1) # í´ë¼ì´ì–¸íŠ¸ ìœ ì§€
    
    except Exception as e:
        logger.error(f"OPC UA ì—°ê²° ë˜ëŠ” êµ¬ë… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        # ğŸ“Œ 5. ìì› í•´ì œ
        if mc is not None:
            mc.set_color(0, 0, 0)
            mc.close()
            logger.info("MyCobot ì •ë¦¬ ì™„ë£Œ.")
        if cap is not None:
            cap.release()
            cv2.destroyAllWindows()
            logger.info("ì¹´ë©”ë¼ ì •ë¦¬ ì™„ë£Œ.")
        logger.info("OPC UA í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ.")

if __name__ == "__main__":
    try:
        # ë¹„ë™ê¸° ë©”ì¸ ë£¨í”„ ì‹¤í–‰
        asyncio.run(arm_subscriber())
    except KeyboardInterrupt:
        logger.info("ì‚¬ìš©ì ì¤‘ë‹¨ (Ctrl+C). í”„ë¡œê·¸ë¨ ì¢…ë£Œ.")
    except Exception as e:
        logger.critical(f"í”„ë¡œê·¸ë¨ ìµœì¢… ì˜¤ë¥˜: {e}")