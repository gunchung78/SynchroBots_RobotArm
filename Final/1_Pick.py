import torch
import cv2
import time
import os
import sys
import csv
import numpy as np
from pymycobot import MyCobot320
from torchvision import transforms
from PIL import Image

# ===============================================
# ğŸ“Œ AI ëª¨ë¸ ì„¤ì • (ì¶”ê°€ëœ ë¶€ë¶„)
# ===============================================
CLASS_NAMES = ["ESP32", "L298N(Motor)", "MB102(Power)"]
NUM_CLASSES = len(CLASS_NAMES)
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MOBILENET_MEAN = [0.485, 0.456, 0.406]
MOBILENET_STD = [0.229, 0.224, 0.225]
# âš ï¸ ëª¨ë¸ íŒŒì¼ ê²½ë¡œë¥¼ ì‹¤ì œ íŒŒì¼ ì´ë¦„ìœ¼ë¡œ ë³€ê²½í•´ì£¼ì„¸ìš”.
MODEL_WEIGHTS_PATH = "checkpoint_mobilenetv3_classifier_e5_acc1.0000.pth"

# ===============================================
# âš™ï¸ MyCobot ë° ë¹„ì „ ì‹œìŠ¤í…œ ì„¤ì • (ê¸°ì¡´ ì„¤ì •)
# ===============================================
PORT = "COM3"
BAUD = 115200

MOVEMENT_SPEED = 70
GRIPPER_SPEED = 50
SEQUENTIAL_MOVE_DELAY = 1.5
GRIPPER_ACTION_DELAY = 1

CAMERA_INDEX = 0
roi_start = (80, 30)
roi_end = (340, 400)
TARGET_CENTER_U = 210
TARGET_CENTER_V = 215

PIXEL_TO_MM_X = 0.526
PIXEL_TO_MM_Y = -0.698

MAX_PIXEL_ERROR = 5
PICK_Z_HEIGHT = 250

GRIPPER_OPEN_VALUE = 85
GRIPPER_CLOSED_VALUE = 25

LOWER_HSV = np.array([0, 0, 0])
UPPER_HSV = np.array([179, 255, 190])

CONVEYOR_CAPTURE_POSE = [0, 0, 90, 0, -90, -90]
ROBOTARM_CAPTURE_POSE = [0, 0, 10, 80, -90, 90]

INTERMEDIATE_POSE_ANGLES = [-17.2, 30.49, 4.48, 53.08, -90.87, -85.86]
ZERO_POSE_ANGLES = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

TEST_PICK_POSE_WIDTH = [-229.30, 20, 183.6, -174.98, 0, 0]
TEST_PICK_POSE_HEIGHT = [-229.30, 7.80, 183.6, -174.98, 0, 90]

DATA_DIR = "capture"
CSV_FILE = os.path.join(DATA_DIR, "pixel_to_mm_data.csv")
COORDINATE_FILE = "pick_coordinate.txt"

# ===============================================
# ğŸ§  AI ëª¨ë¸ í•¨ìˆ˜ (ì¶”ê°€ëœ ë¶€ë¶„)
# ===============================================

# ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (í•™ìŠµ ì‹œ ì‚¬ìš©í•œ ì •ê·œí™”/ë¦¬ì‚¬ì´ì¦ˆì™€ ë™ì¼í•´ì•¼ í•¨)
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=MOBILENET_MEAN, std=MOBILENET_STD)
])

def load_model(model_path, num_classes):
    """í•™ìŠµëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•˜ê³  í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •í•©ë‹ˆë‹¤."""
    try:
        # models.mobilenet_v3_small í•¨ìˆ˜ë¥¼ ì¬ì‚¬ìš©
        model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v3_small', weights=None)
        
        # ìµœì¢… ë¶„ë¥˜ì¸µ(Classifier) ì¬ì •ì˜
        in_features = model.classifier[-1].in_features
        model.classifier[-1] = torch.nn.Linear(in_features, num_classes)
        
        # ì €ì¥ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
        model.load_state_dict(torch.load(model_path, map_location=DEVICE))
        model.to(DEVICE)
        model.eval() # í‰ê°€ ëª¨ë“œ ì„¤ì •
        print(f"âœ… AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path} ({DEVICE})")
        return model
    except FileNotFoundError:
        print(f"\nâŒ ì˜¤ë¥˜: ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼({model_path})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

def classify_object(model, transform, cropped_img):
    """í¬ë¡­ëœ ì´ë¯¸ì§€ë¡œ ê°ì²´ ë¶„ë¥˜ ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    if cropped_img is None or cropped_img.size == 0:
        return "Unknown", 0.0

    # OpenCV (BGR) -> RGB
    rgb_frame = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB)
    
    # NumPy ë°°ì—´ -> PIL Image -> Tensorë¡œ ë³€í™˜ ë° ì •ê·œí™”
    pil_image = Image.fromarray(rgb_frame)
    input_tensor = transform(pil_image).unsqueeze(0).to(DEVICE)
    
    with torch.no_grad():
        outputs = model(input_tensor)
        probabilities = torch.nn.functional.softmax(outputs, dim=1)
        conf_score, predicted_idx = torch.max(probabilities, 1)
        
        predicted_class = CLASS_NAMES[predicted_idx.item()]
        confidence = conf_score.item()
        
    return predicted_class, confidence

# ===============================================
# ğŸ› ï¸ ë¡œë´‡ ë° ë¹„ì „ ì œì–´ í•¨ìˆ˜ (ìˆ˜ì •ëœ ë¶€ë¶„ í¬í•¨)
# ===============================================

def convert_pixel_to_robot_move(current_center_u, current_center_v):
    global TARGET_CENTER_U, TARGET_CENTER_V, PIXEL_TO_MM_X, PIXEL_TO_MM_Y
    
    delta_u_pixel = current_center_u - TARGET_CENTER_U
    delta_v_pixel = current_center_v - TARGET_CENTER_V
    
    delta_X_mm = delta_u_pixel * PIXEL_TO_MM_X
    delta_Y_mm = delta_v_pixel * PIXEL_TO_MM_Y
    
    final_delta_X = -delta_X_mm
    final_delta_Y = -delta_Y_mm
    
    return final_delta_X, final_delta_Y, delta_u_pixel, delta_v_pixel

def find_object_center(frame):
    """
    ë¬¼ì²´ ì¤‘ì‹¬ ì¢Œí‘œë¥¼ ì°¾ê³ , ë¬¼ì²´ ì˜ì—­ì„ í¬ë¡­í•œ ì´ë¯¸ì§€ë¥¼ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤. (ìˆ˜ì •ë¨)
    """
    global LOWER_HSV, UPPER_HSV, roi_start, roi_end
    
    hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    color_mask_full = cv2.inRange(hsv_frame, LOWER_HSV, UPPER_HSV)
    
    roi_mask = np.zeros(color_mask_full.shape, dtype=np.uint8)
    roi_mask[roi_start[1]:roi_end[1], roi_start[0]:roi_end[0]] = 255
    
    color_mask = cv2.bitwise_and(color_mask_full, color_mask_full, mask=roi_mask)
    
    kernel = np.ones((5, 5), np.uint8) 
    color_mask = cv2.erode(color_mask, kernel, iterations=1)
    color_mask = cv2.dilate(color_mask, kernel, iterations=1)

    inverted_mask = cv2.bitwise_not(color_mask)
    final_mask = cv2.bitwise_and(inverted_mask, inverted_mask, mask=roi_mask)
    
    # ë””ë²„ê¹…ìš© ë§ˆìŠ¤í¬ í™”ë©´ í‘œì‹œ
    cv2.imshow('Masked (Final Target)', final_mask)
    
    contours, _ = cv2.findContours(final_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    if contours:
        largest_contour = max(contours, key=cv2.contourArea)
        
        if cv2.contourArea(largest_contour) > 1000:
            M = cv2.moments(largest_contour)
            if M["m00"] != 0:
                center_x = int(M["m10"] / M["m00"])
                center_y = int(M["m01"] / M["m00"])
                
                rect = cv2.minAreaRect(largest_contour)
                
                # ë¬¼ì²´ ì˜ì—­ì˜ Bounding Box ì¢Œí‘œ (AI ì¶”ë¡ ì„ ìœ„í•œ í¬ë¡­ ì˜ì—­)
                x, y, w, h = cv2.boundingRect(largest_contour)
                
                # ì´ë¯¸ì§€ ê²½ê³„ë¥¼ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡ í´ë¨í”„
                x = max(0, x - 10) 
                y = max(0, y - 10)
                x_end = min(frame.shape[1], x + w + 20)
                y_end = min(frame.shape[0], y + h + 20)
                
                # ë¬¼ì²´ ì˜ì—­ í¬ë¡­
                cropped_object_img = frame[y:y_end, x:x_end]
                
                return (center_x, center_y, largest_contour, rect, cropped_object_img) # í¬ë¡­ëœ ì´ë¯¸ì§€ ì¶”ê°€ ë°˜í™˜
            
    return (None, None, None, None, None)

def pick_and_place_vision_guided(mc, cap, frame, ai_model):
    """
    Vision-Guided í”½ì—… ë¡œì§ì— AI ê°ì²´ ë¶„ë¥˜ ê¸°ëŠ¥ì„ ì¶”ê°€í•©ë‹ˆë‹¤. (ìˆ˜ì •ë¨)
    """
    global SEQUENTIAL_MOVE_DELAY, MOVEMENT_SPEED, GRIPPER_OPEN_VALUE, GRIPPER_CLOSED_VALUE, GRIPPER_SPEED, GRIPPER_ACTION_DELAY, TEST_PICK_POSE_WIDTH, TEST_PICK_POSE_HEIGHT

    center_x, center_y, largest_contour, rect, cropped_img = find_object_center(frame) # í¬ë¡­ëœ ì´ë¯¸ì§€ ìˆ˜ì‹ 

    if rect is None:
        print("âŒ ë¬¼ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í”½ì—… ì¤‘ë‹¨.")
        return False
        
    (center_u, center_v), (w, h), angle = rect
    
    # ğŸ“Œ AI ê°ì²´ ë¶„ë¥˜ ìˆ˜í–‰ (ì¶”ê°€ëœ ë¶€ë¶„)
    predicted_class, confidence = classify_object(ai_model, transform, cropped_img)
    
    print(f"\nğŸ§  AI ë¶„ë¥˜ ê²°ê³¼: **{predicted_class}** (ì‹ ë¢°ë„: {confidence*100:.2f}%)")
    
    # í”½ì—… ìì„¸ ê²°ì • (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    if w > h:
        target_pose = list(TEST_PICK_POSE_WIDTH)
        print(f"ğŸ“ ë¬¼ì²´ ì¥ì¶•: ê°€ë¡œ (w={w:.2f} > h={h:.2f}). ìµœì¢… Pose: TEST_PICK_POSE_WIDTH ì„ íƒ.")
    else: 
        target_pose = list(TEST_PICK_POSE_HEIGHT)
        print(f"ğŸ“ ë¬¼ì²´ ì¥ì¶•: ì„¸ë¡œ (h={h:.2f} >= w={w:.2f}). ìµœì¢… Pose: TEST_PICK_POSE_HEIGHT ì„ íƒ.")
        
    # í”½ì…€-ë¡œë´‡ ì¢Œí‘œ ë³€í™˜ ë° ì˜¤ì°¨ ê³„ì‚° (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    delta_X, delta_Y, delta_u_pixel, delta_v_pixel = convert_pixel_to_robot_move(center_x, center_y)
    error = np.sqrt(delta_u_pixel**2 + delta_v_pixel**2)
    
    print(f"ğŸ” í”½ì…€ ì˜¤ì°¨: {error:.2f} í”½ì…€. ë¡œë´‡ ë³´ì • ì´ë™ëŸ‰: (X: {delta_X:.2f}mm, Y: {delta_Y:.2f}mm)")
    
    # í”½ì—… ì¢Œí‘œ ë³´ì •
    target_pose[0] += delta_X
    target_pose[1] += delta_Y
    
    # ë¡œë´‡ ì´ë™ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    safe_pose = list(target_pose)
    safe_pose[2] += 50 
    
    mc.send_coords(safe_pose, MOVEMENT_SPEED)
    time.sleep(SEQUENTIAL_MOVE_DELAY)

    print(f"\nâ¬‡ï¸ í”½ì—… ì‹œì‘: X:{target_pose[0]:.2f}, Y:{target_pose[1]:.2f} (Z:{target_pose[2]:.2f}) í•˜ê°•.")
    mc.send_coords(target_pose, MOVEMENT_SPEED - 30)
    time.sleep(SEQUENTIAL_MOVE_DELAY)
    
    mc.set_gripper_value(GRIPPER_CLOSED_VALUE, GRIPPER_SPEED)
    time.sleep(GRIPPER_ACTION_DELAY)
    
    target_pose[2] += 100
    mc.send_coords(target_pose, MOVEMENT_SPEED)
    time.sleep(SEQUENTIAL_MOVE_DELAY)
    
    print("âœ… í”½ì—… ë° ì•ˆì „ ë†’ì´ ë³µê·€ ì™„ë£Œ.")
    
    # ğŸ“Œ ë¶„ë¥˜ ê²°ê³¼ì— ë”°ë¼ ë‹¤ìŒ ë™ì‘ì„ ìˆ˜í–‰í•˜ëŠ” ë¡œì§ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # ì˜ˆ: if predicted_class == "ESP32": place_at_A() 
    #     elif predicted_class == "L298N(Motor)": place_at_B()
    
    return True

def load_and_move_coords(mc, file_path):
    global MOVEMENT_SPEED, SEQUENTIAL_MOVE_DELAY
    
    print(f"\nğŸ“ {file_path} íŒŒì¼ì—ì„œ ì¢Œí‘œ ë¡œë”© ì‹œì‘...")
    
    try:
        with open(file_path, 'r') as f:
            content = f.read().strip()
            coords_str = content.strip('[]').split(', ')
            
            target_coords = [float(x) for x in coords_str if x]
            
            if len(target_coords) == 6:
                print(f"âœ… ì¢Œí‘œ ë¡œë”© ì„±ê³µ: {target_coords}")
                
                mc.send_angles(INTERMEDIATE_POSE_ANGLES, MOVEMENT_SPEED)
                time.sleep(SEQUENTIAL_MOVE_DELAY)
                
                mc.send_coords(target_coords, MOVEMENT_SPEED)
                time.sleep(SEQUENTIAL_MOVE_DELAY)
                
                print("ğŸš€ íŒŒì¼ì—ì„œ ë¡œë”©ëœ ì¢Œí‘œë¡œ ì´ë™ ì™„ë£Œ.")
            else:
                print(f"âŒ ì˜¤ë¥˜: íŒŒì¼ ë‚´ìš©ì´ 6ê°œì˜ ì¢Œí‘œê°€ ì•„ë‹™ë‹ˆë‹¤. ë‚´ìš©: {content}")
                
    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: '{file_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ìƒì„±í•´ì£¼ì„¸ìš”.")
    except ValueError as e:
        print(f"âŒ ì˜¤ë¥˜: íŒŒì¼ ë‚´ìš© ë³€í™˜ ì¤‘ ë¬¸ì œ ë°œìƒ (ìˆ«ì í˜•ì‹ í™•ì¸ í•„ìš”). ì˜¤ë¥˜: {e}")
    except Exception as e:
        print(f"âŒ ë¡œë´‡ ì´ë™ ì¤‘ í†µì‹  ì˜¤ë¥˜ ë°œìƒ: {e}")

# ===============================================
# ğŸš€ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (AI ëª¨ë¸ ë¡œë“œ ì¶”ê°€)
# ===============================================

def main():
    
    # ğŸ“Œ AI ëª¨ë¸ ë¡œë“œ (ì¶”ê°€ëœ ë¶€ë¶„)
    ai_model = load_model(MODEL_WEIGHTS_PATH, NUM_CLASSES)
    
    try:
        mc = MyCobot320(PORT, BAUD)
        mc.power_on()
        print(f"\nğŸ¤– MyCobot ì—°ê²° ì„±ê³µ: {PORT}. ì´ˆê¸° ìƒíƒœ: íŒŒì›Œ ON (ì„œë³´ ì ê¸ˆ)")

        mc.set_gripper_mode(0)
        mc.init_electric_gripper()
        time.sleep(2)
        mc.set_electric_gripper(0)
        
        mc.set_gripper_value(GRIPPER_OPEN_VALUE, GRIPPER_SPEED)
        time.sleep(GRIPPER_ACTION_DELAY)
        print(f"âœ… ê·¸ë¦¬í¼ ì´ˆê¸°í™” ì™„ë£Œ. ìœ„ì¹˜: **{GRIPPER_OPEN_VALUE} (ì—´ë¦¼)**.")
        
    except Exception as e:
        print(f"\nâŒ MyCobot ì—°ê²° ì‹¤íŒ¨ ({PORT}): {e}")
        sys.exit(1)

    cap = cv2.VideoCapture(CAMERA_INDEX, cv2.CAP_DSHOW)
    if not cap.isOpened():
        print(f"\nâŒ ì¹´ë©”ë¼ ì¸ë±ìŠ¤ {CAMERA_INDEX}ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        mc.close()
        sys.exit(1)
    
    os.makedirs(DATA_DIR, exist_ok=True)
    if not os.path.exists(CSV_FILE):
        with open(CSV_FILE, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['Timestamp', 'Target_Center_U', 'Target_Center_V', 'Robot_Coord_X', 'Robot_Coord_Y'])
        print(f"âœ… ë°ì´í„° ê¸°ë¡ íŒŒì¼ ìƒì„± ì™„ë£Œ: {CSV_FILE}")

    last_center_u = None
    last_center_v = None

    print(f"âœ… í˜„ì¬ ì¹´ë©”ë¼ ì°½ í¬ê¸°: {int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))} x {int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))} í”½ì…€")
    print("\n--- ğŸ”‘ ë¡œë´‡ ì œì–´ ê°€ì´ë“œ ---")
    print(" [q]: ì¢…ë£Œ | [s]: í‹°ì¹­ ì‹œì‘(ì„œë³´ í•´ì œ) | [e]: í‹°ì¹­ ì¢…ë£Œ(ì„œë³´ ì ê¸ˆ)")
    print(" [0]: 0ë„ ìì„¸ | [1]: ì»¨ë² ì´ì–´ ìº¡ì²˜ ìì„¸ | [2]: í”½ì—… ìì„¸ (í…ŒìŠ¤íŠ¸)")
    print(" [3]: ë¡œë´‡íŒ” ìœ„ ìº¡ì²˜ ìì„¸ | [4]: Vision-Guided í”½ì—… | [5]: ê¸°ì¤€ ì¢Œí‘œ ì´ë™")
    print(" [j]: Joint ê°’ ì½ê¸° | [k]: Coordinates ì½ê¸° | [g/h]: ê·¸ë¦¬í¼ ë‹«ê¸°/ì—´ê¸°")
    print(" [c]: í˜„ì¬ í™”ë©´ ìº¡ì²˜ ë° ì¢Œí‘œ ê¸°ë¡")
    print(f" [r]: {COORDINATE_FILE} íŒŒì¼ì˜ ì¢Œí‘œ ë¡œë“œ ë° ì´ë™")
    print(" [w/x]: X+1mm / X-1mm ì´ë™ | [d/a]: Y+1mm / Y-1mm ì´ë™")
    print("----------------------------")

    while True:
        ret, frame = cap.read()
        if not ret:
            time.sleep(0.1)
            continue
        
        # ğŸ“Œ find_object_center í•¨ìˆ˜ì—ì„œ í¬ë¡­ëœ ì´ë¯¸ì§€ë¥¼ í•¨ê»˜ ìˆ˜ì‹  (ìˆ˜ì •ë¨)
        center_x, center_y, largest_contour, rect, cropped_img = find_object_center(frame.copy())
        
        # ... (ë‚˜ë¨¸ì§€ ì‹œê°í™” ë¡œì§ì€ ìœ ì§€) ...

        roi_center_x, roi_center_y = (roi_start[0] + roi_end[0]) // 2, (roi_start[1] + roi_end[1]) // 2
        cv2.rectangle(frame, roi_start, roi_end, (255, 255, 255), 2)
        cv2.circle(frame, (roi_center_x, roi_center_y), 5, (0, 0, 0), -1) 
        cv2.putText(frame, "ROI / Target", (roi_center_x + 10, roi_center_y + 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        if center_x is not None:
            last_center_u, last_center_v = center_x, center_y
            
            x, y, w, h = cv2.boundingRect(largest_contour)
            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
            cv2.circle(frame, (center_x, center_y), 5, (0, 255, 0), -1) 
            
            # ğŸ“Œ ì‹œê°í™”ì— AI ë¶„ë¥˜ ê²°ê³¼ ì¶”ê°€
            predicted_class, confidence = classify_object(ai_model, transform, cropped_img)
            
            cv2.putText(frame, f"Class: {predicted_class}", 
                        (roi_center_x - 200, roi_center_y + 200), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
            cv2.putText(frame, f"Conf: {confidence*100:.2f}%", 
                        (roi_center_x - 200, roi_center_y + 220), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
            
            cv2.putText(frame, f"Detected U(X): {center_x}, Detected V(Y): {center_y}", 
                        (roi_center_x - 200, roi_center_y + 240), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)
            cv2.putText(frame, f"Diff. U(X): {(roi_center_x-center_x)}, Diff. V(Y): {(roi_center_y-center_y)}", 
                        (roi_center_x - 200, roi_center_y + 260), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)
        else:
            cv2.putText(frame, "Target Not Found", (roi_center_x - 310, roi_center_y + 190), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)

        cv2.imshow('MyCobot Pick Task', frame)

        key = cv2.waitKey(1) & 0xFF

        if key == ord('q'):
            print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ ì¢…ë£Œ ëª…ë ¹ ìˆ˜ì‹ . ìì›ì„ í•´ì œí•©ë‹ˆë‹¤...")
            break
            
        elif key == ord('r'):
            load_and_move_coords(mc, COORDINATE_FILE)
            
        elif key == ord('4'):
            print("\nâœ¨ **Vision-Guided Pick Task ì‹œì‘**")
            ret, current_frame = cap.read()
            if ret:
                # ğŸ“Œ AI ëª¨ë¸ì„ ì¸ìˆ˜ë¡œ ì „ë‹¬ (ìˆ˜ì •ë¨)
                success = pick_and_place_vision_guided(mc, cap, current_frame, ai_model) 
                if success:
                    print("ğŸ‘ í”½ì—… íƒœìŠ¤í¬ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ.")
                else:
                    print("ğŸ˜­ í”½ì—… íƒœìŠ¤í¬ ì‹¤íŒ¨.")
            else:
                print("âŒ ì¹´ë©”ë¼ í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨.")
                
        # (ë‚˜ë¨¸ì§€ ë¡œì§... í‚¤ ì…ë ¥, ë¡œë´‡ ì œì–´, ì¢Œí‘œ ê¸°ë¡ ë“±)

        elif key == ord('s'):
            print("\nâ–¶ï¸ **í‹°ì¹­ ëª¨ë“œ ì‹œì‘** (ëª¨ë“  ì„œë³´ ì ê¸ˆ í•´ì œ, ìˆ˜ë™ ì œì–´ ê°€ëŠ¥)")
            mc.release_all_servos()
            
        elif key == ord('e'):
            print("\nâ¸ï¸ **í‹°ì¹­ ëª¨ë“œ ì¢…ë£Œ** (ëª¨ë“  ì„œë³´ ì ê¸ˆ, ë¡œë´‡ ì›€ì§ì„ ê³ ì •)")
            mc.power_on()

        elif key in [ord('w'), ord('x'), ord('a'), ord('d')]:
            current_coords = mc.get_coords()
            
            if not isinstance(current_coords, list) or all(c == -1 for c in current_coords):
                current_coords = list(TEST_PICK_POSE_WIDTH)
                print("âš ï¸ ë¡œë´‡ ì¢Œí‘œë¥¼ ì½ì„ ìˆ˜ ì—†ì–´ ê¸°ì¤€ ì¢Œí‘œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            else:
                current_coords = list(current_coords) 
            
            move_x, move_y = 0.0, 0.0
            axis_name = ""
            
            if key == ord('w'):
                move_x = 5
                axis_name = "X + 5mm"
            elif key == ord('x'):
                move_x = -5
                axis_name = "X - 5mm"
            elif key == ord('d'): 
                move_y = 5
                axis_name = "Y + 5mm"
            elif key == ord('a'): 
                move_y = -5
                axis_name = "Y - 5mm"
            
            if axis_name:
                current_coords[0] += move_x
                current_coords[1] += move_y
                
                mc.send_coords(current_coords, MOVEMENT_SPEED - 30)
                time.sleep(0.1)
                
                print(f"enâ¡ï¸ ì¦ë¶„ ì´ë™ ({axis_name}): ìƒˆë¡œìš´ ì¢Œí‘œ (X:{current_coords[0]:.2f}, Y:{current_coords[1]:.2f})")

        elif key == ord('0'):
            print(f"\nğŸ”„ ë¡œë´‡ì„ 0ë„ ìì„¸ ì´ë™ ì‹œì‘...")
            mc.set_gripper_value(GRIPPER_OPEN_VALUE, GRIPPER_SPEED) 
            mc.send_angles(INTERMEDIATE_POSE_ANGLES, MOVEMENT_SPEED)
            time.sleep(SEQUENTIAL_MOVE_DELAY)
            mc.send_angles(ZERO_POSE_ANGLES, MOVEMENT_SPEED)
            print("âœ… 0ë„ ìì„¸ ì´ë™ ì™„ë£Œ.")
        
        elif key == ord('1'):
            print(f"\nğŸš€ ì»¨ë² ì´ì–´ ìº¡ì²˜ ìì„¸ ({CONVEYOR_CAPTURE_POSE})ë¡œ ì´ë™ ì‹œì‘...")
            mc.send_angles(INTERMEDIATE_POSE_ANGLES, MOVEMENT_SPEED)
            time.sleep(SEQUENTIAL_MOVE_DELAY)
            mc.send_angles(CONVEYOR_CAPTURE_POSE, MOVEMENT_SPEED)
            time.sleep(SEQUENTIAL_MOVE_DELAY)
            print("âœ… CONVEYOR_CAPTURE_POSE ì´ë™ ì™„ë£Œ.")
            
        elif key == ord('2'):
            print(f"\nâ¬‡ï¸ í…ŒìŠ¤íŠ¸ í”½ì—… ê°€ë¡œ ìì„¸ ({TEST_PICK_POSE_WIDTH})ë¡œ ì´ë™ ì‹œì‘...")
            mc.send_angles(INTERMEDIATE_POSE_ANGLES, MOVEMENT_SPEED)
            time.sleep(SEQUENTIAL_MOVE_DELAY)
            mc.send_coords(TEST_PICK_POSE_WIDTH, MOVEMENT_SPEED) 
            print("âœ… TEST_PICK_POSE_WIDTH ì´ë™ ì™„ë£Œ.")
        
        elif key == ord('3'):
            print(f"\nâ¬‡ï¸ í…ŒìŠ¤íŠ¸ í”½ì—… ì„¸ë¡œ ìì„¸ ({TEST_PICK_POSE_HEIGHT})ë¡œ ì´ë™ ì‹œì‘...")
            mc.send_angles(INTERMEDIATE_POSE_ANGLES, MOVEMENT_SPEED)
            time.sleep(SEQUENTIAL_MOVE_DELAY)
            mc.send_coords(TEST_PICK_POSE_HEIGHT, MOVEMENT_SPEED) 
            print("âœ… TEST_PICK_POSE_HEIGHT ì„¸ë¡œ ì´ë™ ì™„ë£Œ.")

        elif key == ord('c'):
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            filename = f"image_{timestamp}.jpg"
            save_path = os.path.join(DATA_DIR, filename)
            
            cv2.imwrite(save_path, frame)
            
            if last_center_u is not None:
                try:
                    current_coords = mc.get_coords()
                    if isinstance(current_coords, list) and not all(c == -1 for c in current_coords):
                        with open(CSV_FILE, 'a', newline='') as f:
                            writer = csv.writer(f)
                            writer.writerow([timestamp, last_center_u, last_center_v, current_coords[0], current_coords[1]])
                        print(f"\nğŸ“¸ ë°ì´í„° ìº¡ì²˜ ì™„ë£Œ: {save_path}. í”½ì…€: ({last_center_u}, {last_center_v}), ë¡œë´‡ X/Y: ({current_coords[0]:.2f}, {current_coords[1]:.2f})")
                    else:
                        print(f"\nâŒ ë¡œë´‡ ì¢Œí‘œë¥¼ ì½ì„ ìˆ˜ ì—†ì–´ í”½ì…€ ë°ì´í„°ë§Œ ìº¡ì²˜ë¨: {save_path}")
                        with open(CSV_FILE, 'a', newline='') as f:
                            csv.writer(f).writerow([timestamp, last_center_u, last_center_v, 'N/A', 'N/A'])
                except Exception as e:
                    print(f"\nâŒ ë¡œë´‡ í†µì‹  ì˜¤ë¥˜ë¡œ ì¢Œí‘œ ê¸°ë¡ ì‹¤íŒ¨: {e}")
            else:
                print(f"\nğŸ”´ ë¬¼ì²´ê°€ ê²€ì¶œë˜ì§€ ì•Šì•„ ìº¡ì²˜ë§Œ ì €ì¥ë¨: {save_path}")

        elif key == ord('j'):
            current_angles = mc.get_angles()
            if isinstance(current_angles, list) and not all(c == -1 for c in current_angles): 
                print(f"\nğŸ“ í˜„ì¬ Joint ê°’ (J1~J6): **{current_angles}**")
            else:
                print("\nâŒ Joint ê°’ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¡œë´‡ ì—°ê²° ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.")

        elif key == ord('k'):
            current_coords = mc.get_coords()
            if isinstance(current_coords, list) and not all(c == -1 for c in current_coords): 
                print(f"\nğŸ—ºï¸ í˜„ì¬ Coordinates (X, Y, Z, R, P, Y): **{current_coords}**") 
            else:
                print("\nâŒ Coordinates ê°’ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¡œë´‡ ì—°ê²° ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        
        elif key == ord('g'):
            print("\nâœŠ ê·¸ë¦¬í¼ ë‹«ëŠ” ì¤‘...")
            mc.set_gripper_value(GRIPPER_CLOSED_VALUE, GRIPPER_SPEED) 
            time.sleep(GRIPPER_ACTION_DELAY)
            print(f"âœ… ê·¸ë¦¬í¼ ë‹«í˜ ì™„ë£Œ (ìœ„ì¹˜: **{GRIPPER_CLOSED_VALUE}**).")
            
        elif key == ord('h'):
            print("\nğŸ‘ ê·¸ë¦¬í¼ ì—¬ëŠ” ì¤‘...")
            mc.set_gripper_value(GRIPPER_OPEN_VALUE, GRIPPER_SPEED)
            time.sleep(GRIPPER_ACTION_DELAY)
            print(f"âœ… ê·¸ë¦¬í¼ ì—´ë¦¼ ì™„ë£Œ (ìœ„ì¹˜: **{GRIPPER_OPEN_VALUE}**).")
        
    print("ğŸ§¹ ìì› í•´ì œ ì¤‘: ì¹´ë©”ë¼ ë° ë¡œë´‡ ì—°ê²° ì¢…ë£Œ...")
    cap.release()
    cv2.destroyAllWindows()
    try:
        mc.close()
    except Exception:
        pass
    print("ğŸ‘ í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì™„ë£Œ.")

if __name__ == "__main__":
    # âš ï¸ PyTorch ë¡œë“œ ì‹œ GPU ë¬¸ì œë‚˜ í™˜ê²½ ë¬¸ì œê°€ ë°œìƒí•  ê²½ìš°, 
    # torch.hub.load() ë¶€ë¶„ì„ pip install torchvisionì„ í†µí•´ importí•œ 
    # models.mobilenet_v3_small ë¡œ ëŒ€ì²´í•˜ëŠ” ê²ƒì„ ê³ ë ¤í•´ ë³´ì„¸ìš”.
    main()