import torch
import cv2
import time
import os
import sys
import numpy as np
import json
import asyncio
from asyncua import ua
from asyncua.client import Client as AsyncuaClient
from pymycobot import MyCobot320
from torchvision import transforms
from PIL import Image
import base64
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ===============================================
# ğŸ“Œ AI ëª¨ë¸ ë° ìƒìˆ˜ ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼)
# ===============================================
CLASS_NAMES = ["ESP32", "L298N(Motor)", "MB102(Power)"]
NUM_CLASSES = len(CLASS_NAMES)
DEVICE = torch.device("cpu") # ë¹„ë™ê¸° í™˜ê²½ì„ ê³ ë ¤í•´ CPUë¡œ ì„¤ì •
MOBILENET_MEAN = [0.485, 0.456, 0.406]
MOBILENET_STD = [0.229, 0.224, 0.225]
MODEL_WEIGHTS_PATH = "checkpoint_mobilenetv3_classifier_e5_acc1.0000.pth"

# âš™ï¸ MyCobot ë° ë¹„ì „ ì‹œìŠ¤í…œ ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼)
PORT = "COM3"
BAUD = 115200

MOVEMENT_SPEED = 70
GRIPPER_SPEED = 50
SEQUENTIAL_MOVE_DELAY = 1.5
GRIPPER_ACTION_DELAY = 1

CAMERA_INDEX = 0
roi_start = (80, 30)
roi_end = (340, 400)
TARGET_CENTER_U = 210
TARGET_CENTER_V = 215

PIXEL_TO_MM_X = 0.526
PIXEL_TO_MM_Y = -0.698

MAX_PIXEL_ERROR = 5
PICK_Z_HEIGHT = 250

GRIPPER_OPEN_VALUE = 85
GRIPPER_CLOSED_VALUE = 25

LOWER_HSV = np.array([0, 0, 0])
UPPER_HSV = np.array([179, 255, 190])

CONVEYOR_CAPTURE_POSE = [0, 0, 90, 0, -90, -90]

INTERMEDIATE_POSE_ANGLES = [-17.2, 30.49, 4.48, 53.08, -90.87, -85.86]
ZERO_POSE_ANGLES = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

TEST_PICK_POSE_WIDTH = [-237.90, 20, 183.6, -174.98, 0, 0]
TEST_PICK_POSE_HEIGHT = [-237.90, 20, 183.6, -174.98, 0, 90]

# --- ğŸ¯ OPC UA ìˆ˜ì‹ /ì†¡ì‹  ì„¤ì • (ìˆ˜ì •ëœ ë¶€ë¶„) ---
OPCUA_READ_URL = "opc.tcp://172.30.1.61:0630/freeopcua/server/"
OPCUA_WRITE_URL = "opc.tcp://172.30.1.61:0630/freeopcua/server/"

# ğŸ“Œ ì½ê¸°(êµ¬ë…) ë…¸ë“œ ID
READ_OBJECT_NODE_ID = "ns=2;i=3" # êµ¬ë… ì‹œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (CMD_NODE_PATHë¡œ ëŒ€ì²´ë¨)
READ_METHOD_NODE_ID = "ns=2;s=read_arm_go_move"

# ğŸ“Œ ì“°ê¸°(Method Call) ë…¸ë“œ ID
WRITE_OBJECT_NODE_ID = "ns=2;i=3" # ë¯¸ì…˜ ìƒíƒœ/ë¹„ì „ ê²°ê³¼ ì „ì†¡ ì‹œ ì‚¬ìš©
WRITE_METHOD_NODE_ID = "ns=2;s=write_send_arm_json" # ë¹„ì „ ê²°ê³¼ ì „ì†¡ Method ID

# ğŸ“Œ ë¯¸ì…˜ ìƒíƒœ ì‘ë‹µ Method ID (ì„ì‹œ: ì‚¬ìš©ì ì½”ë“œì—ëŠ” ì—†ì§€ë§Œ, ì‘ë‹µì„ ìœ„í•´ 13ë²ˆì„ ê°€ì •í•˜ê±°ë‚˜, 24ë²ˆ ì¬ì‚¬ìš©)
# send_mission_stateê°€ write_vision_resultì™€ ë™ì¼í•œ ë…¸ë“œë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒìœ¼ë¡œ ê°€ì •í•˜ê³ ,
# ë…¸ë“œ IDë¥¼ ëª…í™•íˆ ë¶„ë¦¬í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.

# CMD_NODE_PATH = [
#     "0:Objects",
#     "2:ARM",
#     "2:read_arm_go_move"
# ]

# ì „ì—­ ê°ì²´ (ë©”ì¸ ë° í•¸ë“¤ëŸ¬ì—ì„œ ì‚¬ìš©)
mc = None
cap = None
ai_model = None
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=MOBILENET_MEAN, std=MOBILENET_STD)
])

# ===============================================
# ğŸ§  AI ëª¨ë¸ í•¨ìˆ˜ (ê¸°ì¡´ê³¼ ë™ì¼)
# ===============================================

def load_model(model_path, num_classes):
    """í•™ìŠµëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•˜ê³  í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •í•©ë‹ˆë‹¤."""
    try:
        # models.mobilenet_v3_small í•¨ìˆ˜ë¥¼ ì¬ì‚¬ìš©
        model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v3_small', weights=None)
        
        # ìµœì¢… ë¶„ë¥˜ì¸µ(Classifier) ì¬ì •ì˜
        in_features = model.classifier[-1].in_features
        model.classifier[-1] = torch.nn.Linear(in_features, num_classes)
        
        # ì €ì¥ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
        model.load_state_dict(torch.load(model_path, map_location=DEVICE))
        model.to(DEVICE)
        model.eval() # í‰ê°€ ëª¨ë“œ ì„¤ì •
        print(f"âœ… AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path} ({DEVICE})")
        return model
    except FileNotFoundError:
        logger.error(f"\nâŒ ì˜¤ë¥˜: ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼({model_path})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        sys.exit(1)
    except Exception as e:
        logger.error(f"\nâŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

def classify_object(model, transform, cropped_img):
    """í¬ë¡­ëœ ì´ë¯¸ì§€ë¡œ ê°ì²´ ë¶„ë¥˜ ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    if cropped_img is None or cropped_img.size == 0:
        return "Unknown", 0.0

    # OpenCV (BGR) -> RGB
    rgb_frame = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB)
    
    # NumPy ë°°ì—´ -> PIL Image -> Tensorë¡œ ë³€í™˜ ë° ì •ê·œí™”
    pil_image = Image.fromarray(rgb_frame)
    input_tensor = transform(pil_image).unsqueeze(0).to(DEVICE)
    
    with torch.no_grad():
        outputs = model(input_tensor)
        probabilities = torch.nn.functional.softmax(outputs, dim=1)
        conf_score, predicted_idx = torch.max(probabilities, 1)
        
        predicted_class = CLASS_NAMES[predicted_idx.item()]
        confidence = conf_score.item()
        
    return predicted_class, confidence

# ===============================================
# ğŸ› ï¸ ë¡œë´‡ ë° ë¹„ì „ ì œì–´ í•¨ìˆ˜ (ê¸°ì¡´ê³¼ ë™ì¼)
# ===============================================

def convert_pixel_to_robot_move(current_center_u, current_center_v):
    global TARGET_CENTER_U, TARGET_CENTER_V, PIXEL_TO_MM_X, PIXEL_TO_MM_Y
    
    delta_u_pixel = current_center_u - TARGET_CENTER_U
    delta_v_pixel = current_center_v - TARGET_CENTER_V
    
    delta_X_mm = delta_u_pixel * PIXEL_TO_MM_X
    delta_Y_mm = delta_v_pixel * PIXEL_TO_MM_Y
    
    final_delta_X = -delta_X_mm
    final_delta_Y = -delta_Y_mm
    
    return final_delta_X, final_delta_Y, delta_u_pixel, delta_v_pixel

def find_object_center(frame):
    """
    ë¬¼ì²´ ì¤‘ì‹¬ ì¢Œí‘œë¥¼ ì°¾ê³ , ë¬¼ì²´ ì˜ì—­ì„ í¬ë¡­í•œ ì´ë¯¸ì§€ë¥¼ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    global LOWER_HSV, UPPER_HSV, roi_start, roi_end
    
    hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    color_mask_full = cv2.inRange(hsv_frame, LOWER_HSV, UPPER_HSV)
    
    roi_mask = np.zeros(color_mask_full.shape, dtype=np.uint8)
    roi_mask[roi_start[1]:roi_end[1], roi_start[0]:roi_end[0]] = 255
    
    color_mask = cv2.bitwise_and(color_mask_full, color_mask_full, mask=roi_mask)
    
    kernel = np.ones((5, 5), np.uint8) 
    color_mask = cv2.erode(color_mask, kernel, iterations=1)
    color_mask = cv2.dilate(color_mask, kernel, iterations=1)

    inverted_mask = cv2.bitwise_not(color_mask)
    final_mask = cv2.bitwise_and(inverted_mask, inverted_mask, mask=roi_mask)
    
    contours, _ = cv2.findContours(final_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    if contours:
        largest_contour = max(contours, key=cv2.contourArea)
        
        if cv2.contourArea(largest_contour) > 1000:
            M = cv2.moments(largest_contour)
            if M["m00"] != 0:
                center_x = int(M["m10"] / M["m00"])
                center_y = int(M["m01"] / M["m00"])
                
                rect = cv2.minAreaRect(largest_contour)
                
                # ë¬¼ì²´ ì˜ì—­ì˜ Bounding Box ì¢Œí‘œ (AI ì¶”ë¡ ì„ ìœ„í•œ í¬ë¡­ ì˜ì—­)
                x, y, w, h = cv2.boundingRect(largest_contour)
                
                # ì´ë¯¸ì§€ ê²½ê³„ë¥¼ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡ í´ë¨í”„
                x = max(0, x - 10) 
                y = max(0, y - 10)
                x_end = min(frame.shape[1], x + w + 20)
                y_end = min(frame.shape[0], y + h + 20)
                
                # ë¬¼ì²´ ì˜ì—­ í¬ë¡­
                cropped_object_img = frame[y:y_end, x:x_end]
                return (center_x, center_y, largest_contour, rect, cropped_object_img) # í¬ë¡­ëœ ì´ë¯¸ì§€ ì¶”ê°€ ë°˜í™˜
            
    return (None, None, None, None, None)


async def pick_and_place_vision_guided(mc, cap, ai_model):
    """
    Vision-Guided í”½ì—… ë¡œì§ì— AI ê°ì²´ ë¶„ë¥˜ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ OPC UAë¡œ ì „ì†¡í•©ë‹ˆë‹¤.
    (ë¹„ë™ê¸° í™˜ê²½ì— ë§ì¶° time.sleep -> asyncio.sleepìœ¼ë¡œ ìˆ˜ì • í•„ìš”)
    """
    global SEQUENTIAL_MOVE_DELAY, MOVEMENT_SPEED, GRIPPER_OPEN_VALUE, GRIPPER_CLOSED_VALUE, GRIPPER_SPEED, GRIPPER_ACTION_DELAY, TEST_PICK_POSE_WIDTH, TEST_PICK_POSE_HEIGHT, transform

    # í˜„ì¬ í”„ë ˆì„ ìº¡ì²˜
    ret, frame = cap.read()
    if not ret:
        logger.error("âŒ ì¹´ë©”ë¼ í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨. í”½ì—… ì¤‘ë‹¨.")
        # ì‹¤íŒ¨ ë³´ê³  ë° ì´ˆê¸° ì¢Œí‘œ ì „ì†¡
        await send_mission_state("arm_mission_failure")
        await send_vision_result(module_type="Unknown", confidence=0.0, pick_coord=[0.0]*6)
        return False, "Unknown", 0.0, [0.0]*6
    
    center_x, center_y, largest_contour, rect, cropped_img = find_object_center(frame)

    if rect is None:
        logger.error("âŒ ë¬¼ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í”½ì—… ì¤‘ë‹¨.")
        # ì‹¤íŒ¨ ë³´ê³  ë° ì´ˆê¸° ì¢Œí‘œ ì „ì†¡ (ì‹¤íŒ¨ ì‹œ)
        await send_mission_state("arm_mission_failure")
        await send_vision_result(module_type="Unknown", confidence=0.0, pick_coord=[0.0]*6)
        return False, "Unknown", 0.0, [0.0]*6
        
    (center_u, center_v), (w, h), angle = rect
    
    # ğŸ“Œ AI ê°ì²´ ë¶„ë¥˜ ìˆ˜í–‰
    predicted_class, confidence = classify_object(ai_model, transform, cropped_img)
    
    print(f"\nğŸ§  AI ë¶„ë¥˜ ê²°ê³¼: **{predicted_class}** (ì‹ ë¢°ë„: {confidence*100:.2f}%)")
    
    # í”½ì—… ìì„¸ ê²°ì • (ê°€ë¡œ/ì„¸ë¡œ)
    if w > h:
        target_pose = list(TEST_PICK_POSE_WIDTH)
        logger.info(f"ğŸ“ ë¬¼ì²´ ì¥ì¶•: ê°€ë¡œ. ìµœì¢… Pose: TEST_PICK_POSE_WIDTH ì„ íƒ.")
    else: 
        target_pose = list(TEST_PICK_POSE_HEIGHT)
        logger.info(f"ğŸ“ ë¬¼ì²´ ì¥ì¶•: ì„¸ë¡œ. ìµœì¢… Pose: TEST_PICK_POSE_HEIGHT ì„ íƒ.")
        
    # ----------------------------------------------------
    # ë¡œë´‡ ì´ë™ ì‹œì‘ (ë™ì‘ ì‹œì‘ ì „ì— ë¶„ë¥˜ ê²°ê³¼ ì „ì†¡í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì )
    # ----------------------------------------------------
    
    # ì‚¬ìš©ì ì§€ì • crop í›„ serverë¡œ ì†¡ì‹ 
    send_img = frame[30:400, 30:340]
    
    # OPC UA ê²°ê³¼ ì „ì†¡ (ë¡œë´‡ ë™ì‘ ì§ì „ì— ì „ì†¡)
    await send_vision_result(
        module_type=predicted_class, 
        confidence=confidence, 
        pick_coord=target_pose,
        image_to_send=send_img
    )

    # ë¡œë´‡ ì´ë™ (time.sleepì„ asyncio.sleepìœ¼ë¡œ ëŒ€ì²´)
    safe_pose = list(target_pose)
    safe_pose[2] += 50 
    
    mc.send_coords(safe_pose, MOVEMENT_SPEED)
    await asyncio.sleep(SEQUENTIAL_MOVE_DELAY)

    logger.info(f"\nâ¬‡ï¸ í”½ì—… ì‹œì‘: X:{target_pose[0]:.2f}, Y:{target_pose[1]:.2f} (Z:{target_pose[2]:.2f}) í•˜ê°•.")
    mc.send_coords(target_pose, MOVEMENT_SPEED - 30)
    await asyncio.sleep(SEQUENTIAL_MOVE_DELAY)
    
    mc.set_gripper_value(GRIPPER_CLOSED_VALUE, GRIPPER_SPEED)
    await asyncio.sleep(GRIPPER_ACTION_DELAY)
    
    target_pose[2] += 100
    mc.send_coords(target_pose, MOVEMENT_SPEED)
    await asyncio.sleep(SEQUENTIAL_MOVE_DELAY)
    
    logger.info("âœ… í”½ì—… ë° ì•ˆì „ ë†’ì´ ë³µê·€ ì™„ë£Œ.")
    
    return True, predicted_class, confidence, target_pose

# ===============================================
# ğŸŒ OPC UA í†µì‹  í•¨ìˆ˜ (READ/WRITE NODE ID ë¶„ë¦¬í•˜ì—¬ ìˆ˜ì •)
# ===============================================

async def send_mission_state(status: str):
    """ë¯¸ì…˜ ìƒíƒœ(arm_mission_success/failure)ë¥¼ ì„œë²„ì— ì†¡ì‹ í•©ë‹ˆë‹¤. (WRITE_OBJECT_NODE_ID, WRITE_METHOD_NODE_ID ì‚¬ìš©)"""
    # âš ï¸ ë¯¸ì…˜ ìƒíƒœ ì „ì†¡ì„ ìœ„í•œ ë³„ë„ì˜ Method IDê°€ ì—†ìœ¼ë¯€ë¡œ
    # READ_METHOD_NODE_ID("ns=2;i=13")ë¥¼ ì‚¬ìš©í•˜ì—¬ Callí•˜ëŠ” ê²ƒìœ¼ë¡œ ê°€ì •í•©ë‹ˆë‹¤.
    
    global OPCUA_WRITE_URL, WRITE_OBJECT_NODE_ID, WRITE_METHOD_NODE_ID 

    # mission_state = { "status": status }
    mission_state = { 
        "module_type": "Mission_State",
        "classification_confidence": 0.0,
        "pick_coord": ["0.00", "0.00", "0.00", "0.00", "0.00", "0.00"],
        "pick_coord_confidence": 0.0,
        "img": "",
        "status": status # ë¯¸ì…˜ ìƒíƒœëŠ” status í•„ë“œì— ë‹´ì•„ ì „ì†¡
    }

    json_str = json.dumps(mission_state)
    
    logger.info(f"OPC UA ë¯¸ì…˜ ìƒíƒœ ì†¡ì‹  ì„œë²„ì— ì—°ê²° ì‹œë„: {OPCUA_WRITE_URL}")
    try:
        async with AsyncuaClient(OPCUA_WRITE_URL) as client:
            obj = client.get_node(WRITE_OBJECT_NODE_ID)
            # âš ï¸ ì´ ë¶€ë¶„ì—ì„œ READ_METHOD_NODE_IDë¥¼ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.
            method_node = client.get_node(WRITE_METHOD_NODE_ID)
            
            print(f"\n[OPC UA WRITE - MISSION_STATE] call_method(status='{status}') (Method: {WRITE_METHOD_NODE_ID})")
            json_variant = ua.Variant(json_str, ua.VariantType.String)

            result_code, result_message = await obj.call_method(
                method_node.nodeid,
                json_variant
            )
            logger.info(f"OPC UA ë¯¸ì…˜ ìƒíƒœ ì†¡ì‹  ì™„ë£Œ. ResultCode: {result_code}")
            return result_code, result_message

    except Exception as e:
        logger.error(f"OPC UA ë¯¸ì…˜ ìƒíƒœ ì†¡ì‹  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return -1, str(e)


async def send_vision_result(module_type: str, confidence: float, pick_coord: list, image_to_send: np.ndarray = None):
    """
    ë¶„ë¥˜ ë° í”½ì—… ê²°ê³¼ë¥¼ JSON í˜•íƒœë¡œ ë¬¶ì–´ OPC UA ì„œë²„ì— ì†¡ì‹ í•©ë‹ˆë‹¤. (WRITE_OBJECT_NODE_ID, WRITE_METHOD_NODE_ID ì‚¬ìš©)
    """
    global OPCUA_WRITE_URL, WRITE_OBJECT_NODE_ID, WRITE_METHOD_NODE_ID
    
    # --- ğŸ“Œ ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ì¸ì½”ë”© ë¡œì§ (ê¸°ì¡´ê³¼ ë™ì¼) ---
    base64_img_str = ""
    if image_to_send is not None:
        try:
            # 1. í•´ìƒë„ ì¶•ì†Œ (ì˜ˆ: 224x224ë¡œ ë¦¬ì‚¬ì´ì¦ˆ)
            resized_img = cv2.resize(image_to_send, (224, 224), interpolation=cv2.INTER_AREA)

            # 2. JPEG ì••ì¶• ì¸ì½”ë”© (ì••ì¶• í’ˆì§ˆ 80 ì„¤ì •)
            encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 80] 
            _, buffer = cv2.imencode('.jpg', resized_img, encode_param)
            
            # 3. Base64 ì¸ì½”ë”© (ë°”ì´ë„ˆë¦¬ ë°ì´í„°ë¥¼ ASCII ë¬¸ìì—´ë¡œ ë³€í™˜)
            base64_img_bytes = base64.b64encode(buffer)
            base64_img_str = base64_img_bytes.decode('utf-8')
            logger.info(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ì¸ì½”ë”© ì™„ë£Œ. Base64 ë¬¸ìì—´ ê¸¸ì´: {len(base64_img_str)}")
            
        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ ì¸ì½”ë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            base64_img_str = "" # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì´ë¯¸ì§€ í•„ë“œë¥¼ ë¹ˆ ë¬¸ìì—´ë¡œ ë‘ 
    
    # ë¹„ì „ ê²°ê³¼ JSON ë°ì´í„° êµ¬ì„±
    vision_result = {
        "module_type": module_type,
        "classification_confidence": confidence,
        "pick_coord": [f"{c:.2f}" for c in pick_coord], # ë¡œë´‡ ì¢Œí‘œë¥¼ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ì „ì†¡
        "pick_coord_confidence": 0.9984073221683503,
        "img": base64_img_str
    }
    json_str = json.dumps(vision_result)

    # ğŸ“Œ ì¶”ê°€ëœ ë¶€ë¶„: í´ë¼ì´ì–¸íŠ¸ê°€ ì‹¤ì œë¡œ ì „ì†¡í•˜ëŠ” JSON ë¬¸ìì—´ ì¶œë ¥
    print("\n========================================================")
    print(f"ğŸš€ [VISION RESULT] í´ë¼ì´ì–¸íŠ¸ê°€ ì„œë²„ë¡œ ì „ì†¡í•˜ëŠ” ìµœì¢… JSON:")
    print(json_str)
    print("========================================================\n")
    
    try:
        async with AsyncuaClient(OPCUA_WRITE_URL) as client:
            # âš ï¸ ì´ ë¶€ë¶„ì—ì„œ WRITE_OBJECT_NODE_ID ì‚¬ìš©
            obj = client.get_node(WRITE_OBJECT_NODE_ID)
            # âš ï¸ ì´ ë¶€ë¶„ì—ì„œ WRITE_METHOD_NODE_ID ì‚¬ìš©
            method_node = client.get_node(WRITE_METHOD_NODE_ID)

            # ğŸ“Œ ê°œì„ : ë¬¸ìì—´ì„ ua.Variant(ua.String)ìœ¼ë¡œ ëª…ì‹œì  ë³€í™˜í•˜ì—¬ ì „ì†¡
            json_variant = ua.Variant(json_str, ua.VariantType.String)

            print(f"\n[OPC UA WRITE - VISION_RESULT] call_method(Module: {module_type}, Conf: {confidence*100:.2f}%) (Method: {WRITE_METHOD_NODE_ID})")
            result_code, result_message = await obj.call_method(
                method_node.nodeid,
                json_variant # ua.Variant ê°ì²´ ì „ì†¡
            )
            logger.info(f"OPC UA ë¹„ì „ ê²°ê³¼ ì†¡ì‹  ì™„ë£Œ. ResultCode: {result_code}")

    except Exception as e:
        logger.error(f"OPC UA ë¹„ì „ ê²°ê³¼ ì†¡ì‹  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


# ----------------------
# OPC UA DataChange êµ¬ë… í•¸ë“¤ëŸ¬ í´ë˜ìŠ¤ (ê¸°ì¡´ê³¼ ë™ì¼)
# ----------------------
class SubHandler:
    
    def __init__(self, mycobot_instance, camera_instance, ai_model_instance):
        self.mc = mycobot_instance
        self.cap = camera_instance
        self.ai_model = ai_model_instance
        logger.info("SubHandler ì´ˆê¸°í™” ì™„ë£Œ.")
        

    def datachange_notification(self, node, val, data):
        """ë°ì´í„° ë³€ê²½ ì•Œë¦¼ ì‹œ í˜¸ì¶œë˜ëŠ” ë¹„ë™ê¸°ì  ì½œë°± í•¨ìˆ˜"""
        # ë¹„ë™ê¸° í•¨ìˆ˜ì¸ execute_command_and_respondë¥¼ ë³„ë„ì˜ íƒœìŠ¤í¬ë¡œ ì‹¤í–‰
        # ë¡œë´‡/ë¹„ì „ ì‘ì—…ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ ì½œë°±ì´ ë¹¨ë¦¬ ëë‚˜ë„ë¡ íƒœìŠ¤í¬ë¡œ ë§Œë“­ë‹ˆë‹¤.
        asyncio.create_task(self.execute_command_and_respond(val))

    async def execute_command_and_respond(self, val):
        """ëª…ë ¹ì„ íŒŒì‹±í•˜ê³  MyCobot ë™ì‘ì„ ìˆ˜í–‰í•œ í›„ ì‘ë‹µí•©ë‹ˆë‹¤."""
        
        print(f"\n[OPC UA READ] ìˆ˜ì‹  ê°’: {val}")

        command = None
        if isinstance(val, str):
            try:
                json_data = json.loads(val)
                if "move_command" in json_data:
                    command = json_data["move_command"]
            except json.JSONDecodeError:
                command = val # Ready ê°™ì€ ì¼ë°˜ ë¬¸ìì—´ë„ commandë¡œ ê°„ì£¼

        
        # 3. MyCobot ë™ì‘ ìˆ˜í–‰ ë° ì‘ë‹µ
        if command and self.mc is not None and self.cap is not None:
            
            if command == "go_home":
                # 1ë²ˆ í‚¤ì™€ ê°™ì€ ë™ì‘: CONVEYOR_CAPTURE_POSEë¡œ ì´ë™
                logger.info("-> MyCobot: go_home ëª…ë ¹ ìˆ˜í–‰ (CONVEYOR_CAPTURE_POSEë¡œ ì´ë™)")
                self.mc.send_angles(INTERMEDIATE_POSE_ANGLES, MOVEMENT_SPEED)
                await asyncio.sleep(SEQUENTIAL_MOVE_DELAY)
                self.mc.send_angles(CONVEYOR_CAPTURE_POSE, MOVEMENT_SPEED)
                await asyncio.sleep(SEQUENTIAL_MOVE_DELAY)
                
                # ë™ì‘ ì™„ë£Œ ë³´ê³ 
                await send_mission_state("arm_mission_success")
            
            elif command == "mission_start":
                # 4ë²ˆ í‚¤ì™€ ê°™ì€ ë™ì‘: Vision-Guided Pick ìˆ˜í–‰
                logger.info("-> MyCobot: mission_start ëª…ë ¹ ìˆ˜í–‰ (Vision-Guided Pick ì‹œì‘)")
                
                # --- ë¯¸ì…˜ ì‹œì‘ ë™ì‘ ---
                success, module_type, confidence, pick_coord = await pick_and_place_vision_guided(self.mc, self.cap, self.ai_model)
                
                # --- ë¯¸ì…˜ ì¢…ë£Œ ---
                if success:
                    logger.info("-> MyCobot: Vision-Guided Pick ì™„ë£Œ. OPC UA ì‘ë‹µ ì†¡ì‹  ì‹œì‘.")
                    await send_mission_state("arm_mission_success")
                else:
                    logger.error("-> MyCobot: Vision-Guided Pick ì‹¤íŒ¨. OPC UA ì‹¤íŒ¨ ë³´ê³  ì†¡ì‹ .")
                    await send_mission_state("arm_mission_failure")
                    
            elif command == "Ready":
                logger.info("-> MyCobot: Ready ìƒíƒœ ìˆ˜ì‹ , ëŒ€ê¸° ì¤‘...")
                
            else:
                logger.warning(f"-> MyCobot: ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹: {command}")
        elif command and (self.mc is None or self.cap is None):
            logger.warning(f"-> MyCobot ë˜ëŠ” ì¹´ë©”ë¼ ì—°ê²° ë¬¸ì œë¡œ '{command}' ëª…ë ¹ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


async def arm_subscriber():
    """
    OPC UA í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‹¤í–‰í•˜ê³  êµ¬ë…ì„ ì„¤ì •í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
    """
    global mc, cap, ai_model

    # ğŸ“Œ 1. AI ëª¨ë¸ ë¡œë“œ (MyCobot/Camera ì „ì— ìˆ˜í–‰)
    ai_model = load_model(MODEL_WEIGHTS_PATH, NUM_CLASSES)
    
    # ğŸ“Œ 2. MyCobot ì—°ê²° ì´ˆê¸°í™”
    try:
        mc = MyCobot320(PORT, BAUD)
        mc.set_color(0, 0, 255) 
        logger.info(f"MyCobot320ì´ {PORT}ì— {BAUD} ì†ë„ë¡œ ì„±ê³µì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ê·¸ë¦¬í¼ ì´ˆê¸°í™” ë¡œì§
        mc.set_gripper_mode(0)
        mc.init_electric_gripper()
        time.sleep(2)
        mc.set_electric_gripper(0)
        mc.set_gripper_value(GRIPPER_OPEN_VALUE, GRIPPER_SPEED, 1) # GRIPPER_OPEN_VALUE (85)ë¡œ ì—´ë¦¼
        time.sleep(2)
        logger.info(f"-> MyCobot320: ì „ê¸° ê·¸ë¦¬í¼ ì´ˆê¸°í™” ì™„ë£Œ ({GRIPPER_OPEN_VALUE} ìœ„ì¹˜ë¡œ ì´ë™).")
        
    except Exception as e:
        logger.error(f"MyCobot320 ì—°ê²° ë˜ëŠ” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        mc = None

    # ğŸ“Œ 3. ì¹´ë©”ë¼ ì—°ê²° ì´ˆê¸°í™”
    cap = cv2.VideoCapture(CAMERA_INDEX, cv2.CAP_DSHOW)
    if not cap.isOpened():
        logger.error(f"ì¹´ë©”ë¼ ì¸ë±ìŠ¤ {CAMERA_INDEX}ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        cap = None

    if mc is None or cap is None:
        logger.error("MyCobot ë˜ëŠ” ì¹´ë©”ë¼ ì—°ê²° ë¬¸ì œë¡œ Vision-Pick ë¯¸ì…˜ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        if mc is not None:
            mc.close()

    # ğŸ“Œ 4. OPC UA ì—°ê²° ë° êµ¬ë… ì„¤ì •
    logger.info(f"OPC UA ìˆ˜ì‹  ì„œë²„ì— ì—°ê²° ì‹œë„: {OPCUA_READ_URL}")

    try:
        async with AsyncuaClient(OPCUA_READ_URL) as client:
            logger.info("OPC UA ìˆ˜ì‹  ì„œë²„ì— ì„±ê³µì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")

            handler = SubHandler(mc, cap, ai_model)
            # êµ¬ë… ê°„ê²© 100ms
            sub = await client.create_subscription(100, handler)

            cmd_node = client.get_node(READ_METHOD_NODE_ID) 

            await sub.subscribe_data_change(cmd_node)
            logger.info(f"ë…¸ë“œ '{READ_METHOD_NODE_ID}' êµ¬ë… ì‹œì‘. ë°ì´í„° ìˆ˜ì‹  ëŒ€ê¸° ì¤‘...")
            while True:
                await asyncio.sleep(1) # í´ë¼ì´ì–¸íŠ¸ ìœ ì§€
    
    except Exception as e:
        logger.error(f"OPC UA ì—°ê²° ë˜ëŠ” êµ¬ë… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        # ğŸ“Œ 5. ìì› í•´ì œ
        if mc is not None:
            mc.set_color(0, 0, 0)
            mc.close()
            logger.info("MyCobot ì •ë¦¬ ì™„ë£Œ.")
        if cap is not None:
            cap.release()
            cv2.destroyAllWindows()
            logger.info("ì¹´ë©”ë¼ ì •ë¦¬ ì™„ë£Œ.")
        logger.info("OPC UA í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ.")


if __name__ == "__main__":
    try:
        # ë¹„ë™ê¸° ë©”ì¸ ë£¨í”„ ì‹¤í–‰
        asyncio.run(arm_subscriber())
    except KeyboardInterrupt:
        logger.info("ì‚¬ìš©ì ì¤‘ë‹¨ (Ctrl+C). í”„ë¡œê·¸ë¨ ì¢…ë£Œ.")
    except Exception as e:
        logger.critical(f"í”„ë¡œê·¸ë¨ ìµœì¢… ì˜¤ë¥˜: {e}")