import torch
import cv2
import time
import os
import sys
import numpy as np
import json
import asyncio
from asyncua import ua
from asyncua.client import Client as AsyncuaClient
from pymycobot import MyCobot320
from torchvision import transforms
from PIL import Image
import base64
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ===============================================
# ğŸ“Œ AI ëª¨ë¸ ë° ìƒìˆ˜ ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼)
# ===============================================
CLASS_NAMES = ["ESP32", "L298N(Motor)", "MB102(Power)"]
NUM_CLASSES = len(CLASS_NAMES)
DEVICE = torch.device("cpu") # ë¹„ë™ê¸° í™˜ê²½ì„ ê³ ë ¤í•´ CPUë¡œ ì„¤ì •
MOBILENET_MEAN = [0.485, 0.456, 0.406]
MOBILENET_STD = [0.229, 0.224, 0.225]
MODEL_WEIGHTS_PATH = "checkpoint_mobilenetv3_classifier_e5_acc1.0000.pth"

# âš™ï¸ MyCobot ë° ë¹„ì „ ì‹œìŠ¤í…œ ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼)
PORT = "COM3"
BAUD = 115200

MOVEMENT_SPEED = 70
GRIPPER_SPEED = 50
SEQUENTIAL_MOVE_DELAY = 3
GRIPPER_ACTION_DELAY = 1

CAMERA_INDEX = 0
roi_start = (0, 0)
roi_end = (640, 360)
TARGET_CENTER_U = 320
TARGET_CENTER_V = 180

PIXEL_TO_MM_X = 0.526
PIXEL_TO_MM_Y = -0.698

MAX_PIXEL_ERROR = 5
PICK_Z_HEIGHT = 260

GRIPPER_OPEN_VALUE = 85
GRIPPER_CLOSED_VALUE = 25

LOWER_HSV = np.array([0, 0, 0])
UPPER_HSV = np.array([179, 255, 190])

CONVEYOR_CAPTURE_POSE = [0, 0, 90, 0, -90, -90]
ROBOTARM_CAPTURE_POSE = [0, 0, 10, 80, -90, 90]

INTERMEDIATE_POSE_ANGLES = [-17.2, 30.49, 4.48, 53.08, -90.87, -85.86]
ZERO_POSE_ANGLES = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

TEST_PICK_POSE_WIDTH = [-237.90, 20, 183.6, -174.98, 0, 0]
TEST_PICK_POSE_HEIGHT = [-237.90, 20, 183.6, -174.98, 0, 90]

# --- Place ë³€ìˆ˜
LOWER_RED_HSV1 = np.array([0, 100, 100])
UPPER_RED_HSV1 = np.array([15, 255, 255])
LOWER_RED_HSV2 = np.array([155, 100, 100])
UPPER_RED_HSV2 = np.array([179, 255, 255])

GLOBAL_TARGET_COORDS = [-114, -195, 250, 177.71, 0.22, 0]
GLOBAL_TARGET_TMP_COORDS = [-150.0, -224.4, 318.1, 176.26, 3.2, 3.02]

# --- ğŸ¯ OPC UA ìˆ˜ì‹ /ì†¡ì‹  ì„¤ì • (ìˆ˜ì •ëœ ë¶€ë¶„) ---
OPCUA_READ_URL = "opc.tcp://172.30.1.61:0630/freeopcua/server/"
OPCUA_WRITE_URL = "opc.tcp://172.30.1.61:0630/freeopcua/server/"

# ğŸ“Œ ì½ê¸°(êµ¬ë…) ë…¸ë“œ ID
READ_OBJECT_NODE_ID = "ns=2;i=3"
READ_METHOD_NODE_ID = "ns=2;s=read_arm_go_move"

# ğŸ“Œ ì“°ê¸°(Method Call) ë…¸ë“œ ID
WRITE_OBJECT_NODE_ID = "ns=2;i=3"
WRITE_METHOD_NODE_ID = "ns=2;s=write_send_arm_json"

# ì „ì—­ ê°ì²´ (ë©”ì¸ ë° í•¸ë“¤ëŸ¬ì—ì„œ ì‚¬ìš©)
mc = None
cap = None
ai_model = None
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=MOBILENET_MEAN, std=MOBILENET_STD)
])

# ===============================================
# ğŸ§  AI ëª¨ë¸ í•¨ìˆ˜ (ê¸°ì¡´ê³¼ ë™ì¼)
# ===============================================

def load_model(model_path, num_classes):
    """í•™ìŠµëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•˜ê³  í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •í•©ë‹ˆë‹¤."""
    try:
        # models.mobilenet_v3_small í•¨ìˆ˜ë¥¼ ì¬ì‚¬ìš©
        model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v3_small', weights=None)
        
        # ìµœì¢… ë¶„ë¥˜ì¸µ(Classifier) ì¬ì •ì˜
        in_features = model.classifier[-1].in_features
        model.classifier[-1] = torch.nn.Linear(in_features, num_classes)
        
        # ì €ì¥ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
        model.load_state_dict(torch.load(model_path, map_location=DEVICE))
        model.to(DEVICE)
        model.eval() # í‰ê°€ ëª¨ë“œ ì„¤ì •
        print(f"âœ… AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path} ({DEVICE})")
        return model
    except FileNotFoundError:
        logger.error(f"\nâŒ ì˜¤ë¥˜: ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼({model_path})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        sys.exit(1)
    except Exception as e:
        logger.error(f"\nâŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

def classify_object(model, transform, cropped_img):
    """í¬ë¡­ëœ ì´ë¯¸ì§€ë¡œ ê°ì²´ ë¶„ë¥˜ ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    if cropped_img is None or cropped_img.size == 0:
        return "Unknown", 0.0

    # OpenCV (BGR) -> RGB
    rgb_frame = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB)
    
    # NumPy ë°°ì—´ -> PIL Image -> Tensorë¡œ ë³€í™˜ ë° ì •ê·œí™”
    pil_image = Image.fromarray(rgb_frame)
    input_tensor = transform(pil_image).unsqueeze(0).to(DEVICE)
    
    with torch.no_grad():
        outputs = model(input_tensor)
        probabilities = torch.nn.functional.softmax(outputs, dim=1)
        conf_score, predicted_idx = torch.max(probabilities, 1)
        
        predicted_class = CLASS_NAMES[predicted_idx.item()]
        confidence = conf_score.item()
        
    return predicted_class, confidence

# ===============================================
# ğŸ› ï¸ ë¡œë´‡ ë° ë¹„ì „ ì œì–´ í•¨ìˆ˜ (ìˆ˜ì • ì—†ìŒ)
# ===============================================

def convert_pixel_to_robot_move(current_center_u, current_center_v):
    global TARGET_CENTER_U, TARGET_CENTER_V, PIXEL_TO_MM_X, PIXEL_TO_MM_Y
    
    delta_u_pixel = current_center_u - TARGET_CENTER_U
    delta_v_pixel = current_center_v - TARGET_CENTER_V
    
    delta_X_mm = delta_u_pixel * PIXEL_TO_MM_X
    delta_Y_mm = delta_v_pixel * PIXEL_TO_MM_Y
    
    final_delta_X = -delta_X_mm
    final_delta_Y = -delta_Y_mm
    
    return final_delta_X, final_delta_Y, delta_u_pixel, delta_v_pixel

def find_object_center(frame):
    """ ë¬¼ì²´ ì¤‘ì‹¬ ì¢Œí‘œë¥¼ ì°¾ê³ , ë¬¼ì²´ ì˜ì—­ì„ í¬ë¡­í•œ ì´ë¯¸ì§€ë¥¼ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤. """
    global LOWER_HSV, UPPER_HSV, roi_start, roi_end
    
    hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    color_mask_full = cv2.inRange(hsv_frame, LOWER_HSV, UPPER_HSV)
    
    roi_mask = np.zeros(color_mask_full.shape, dtype=np.uint8)
    roi_mask[roi_start[1]:roi_end[1], roi_start[0]:roi_end[0]] = 255
    
    color_mask = cv2.bitwise_and(color_mask_full, color_mask_full, mask=roi_mask)
    
    kernel = np.ones((5, 5), np.uint8) 
    color_mask = cv2.erode(color_mask, kernel, iterations=1)
    color_mask = cv2.dilate(color_mask, kernel, iterations=1)

    inverted_mask = cv2.bitwise_not(color_mask)
    final_mask = cv2.bitwise_and(inverted_mask, inverted_mask, mask=roi_mask)
    
    contours, _ = cv2.findContours(final_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    if contours:
        largest_contour = max(contours, key=cv2.contourArea)
        
        if cv2.contourArea(largest_contour) > 1000:
            M = cv2.moments(largest_contour)
            if M["m00"] != 0:
                center_x = int(M["m10"] / M["m00"])
                center_y = int(M["m01"] / M["m00"])
                
                rect = cv2.minAreaRect(largest_contour)
                
                # ë¬¼ì²´ ì˜ì—­ì˜ Bounding Box ì¢Œí‘œ (AI ì¶”ë¡ ì„ ìœ„í•œ í¬ë¡­ ì˜ì—­)
                x, y, w, h = cv2.boundingRect(largest_contour)
                
                # ì´ë¯¸ì§€ ê²½ê³„ë¥¼ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡ í´ë¨í”„
                x = max(0, x - 10) 
                y = max(0, y - 10)
                x_end = min(frame.shape[1], x + w + 20)
                y_end = min(frame.shape[0], y + h + 20)
                
                # ë¬¼ì²´ ì˜ì—­ í¬ë¡­
                cropped_object_img = frame[y:y_end, x:x_end]
                return (center_x, center_y, largest_contour, rect, cropped_object_img) # í¬ë¡­ëœ ì´ë¯¸ì§€ ì¶”ê°€ ë°˜í™˜
            
    return (None, None, None, None, None)

def find_red_center(frame):
    """ ì£¼ì–´ì§„ ì´ë¯¸ì§€ í”„ë ˆì„ì—ì„œ ê°€ì¥ í° ë¹¨ê°„ìƒ‰ ì˜ì—­ì˜ ì¤‘ì‹¬ í”½ì…€ (u, v)ë¥¼ ì°¾ê³  ìœ¤ê³½ì„ ì„ ë°˜í™˜í•©ë‹ˆë‹¤. """
    
    hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    
    # ë‘ ê°œì˜ ë¹¨ê°„ìƒ‰ ë²”ìœ„ ë§ˆìŠ¤í¬ë¥¼ í•©ì¹˜ê¸° (0~10ë„, 160~179ë„)
    mask1 = cv2.inRange(hsv_frame, LOWER_RED_HSV1, UPPER_RED_HSV1)
    mask2 = cv2.inRange(hsv_frame, LOWER_RED_HSV2, UPPER_RED_HSV2)
    red_mask = cv2.bitwise_or(mask1, mask2)
    
    # ìœ¤ê³½ì„  ì°¾ê¸°
    contours, _ = cv2.findContours(red_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    if contours:
        # ê°€ì¥ í° ìœ¤ê³½ì„  ì„ íƒ
        largest_contour = max(contours, key=cv2.contourArea)
        
        if cv2.contourArea(largest_contour) > 50: # ìµœì†Œ ë©´ì  í•„í„°ë§
            M = cv2.moments(largest_contour)
            if M["m00"] != 0:
                center_x = int(M["m10"] / M["m00"])
                center_y = int(M["m01"] / M["m00"])
                return (center_x, center_y, largest_contour)
                
    return (None, None, None) # ê²€ì¶œ ì‹¤íŒ¨ ì‹œ None ë°˜í™˜

# ===============================================
# ğŸ¤– ë¡œë´‡ ë¹„ë™ê¸° í—¬í¼ í•¨ìˆ˜ (ì•ˆì „ ë§ˆì§„ ì¶”ê°€)
# ===============================================

async def wait_until_stopped(mc, safety_delay=2.0):
    """ ë¡œë´‡ì´ ì›€ì§ì„ì„ ì™„ì „íˆ ë©ˆì¶”ê³  ì•ˆì „ ë§ˆì§„ ì‹œê°„ë§Œí¼ ëŒ€ê¸°í•©ë‹ˆë‹¤. """
    logger.info("... ë¡œë´‡ ì›€ì§ì„ ì™„ë£Œ ëŒ€ê¸° ì¤‘ (is_moving ì²´í¬)...")

    # 1. is_movingì´ Falseë¥¼ ë°˜í™˜í•  ë•Œê¹Œì§€ ëŒ€ê¸°
    while await asyncio.to_thread(mc.is_moving):
        await asyncio.sleep(0.2)
        
    # 2. ì›€ì§ì„ì´ ë©ˆì¶˜ í›„, ë¡œë´‡ ì œì–´ê¸°ê°€ ë‹¤ìŒ ëª…ë ¹ì„ ë°›ì„ ì¤€ë¹„ê°€ ë  ì‹œê°„ì„ í™•ë³´ (ì•ˆì „ ë§ˆì§„)
    logger.info(f"... ì›€ì§ì„ ì¤‘ì§€ í™•ì¸. ì•ˆì „ ë§ˆì§„ {safety_delay}ì´ˆ ì¶”ê°€ ëŒ€ê¸°...")
    await asyncio.sleep(safety_delay) 
    
    return True

async def place_coords_calculator(cap):
    """ 
    [Vision-Guided] ê²€ì¶œëœ ë¹¨ê°„ìƒ‰ êµ¬ì—­ì˜ ì¤‘ì‹¬ì„ ê¸°ì¤€ìœ¼ë¡œ ë°°ì¹˜í•  ìµœì¢… ëª©í‘œ ì¢Œí‘œë¥¼ ê³„ì‚°í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print("â˜†â˜…â˜†â˜…â˜†â˜…â˜†â˜… place_coords_calculator: Place ëª©í‘œ ì¢Œí‘œ ê³„ì‚° ì‹œì‘")
    global GLOBAL_TARGET_COORDS, MOVEMENT_SPEED, PICK_Z_HEIGHT
    
    # 1. ì´ë¯¸ì§€ ìº¡ì²˜ ë° ì¤‘ì‹¬ ì°¾ê¸°
    ret, frame = cap.read()
    if not ret:
        print("âŒ í”„ë ˆì„ ìˆ˜ì‹  ì‹¤íŒ¨. ì¢Œí‘œ ê³„ì‚° ì¤‘ì§€.")
        return False, None
    
    DATA_DIR = "place_capture"
    filename = f"place_calc_frame.jpg"
    save_path = os.path.join(DATA_DIR, filename)
    cv2.imwrite(save_path, frame)

    print(f"ğŸ–¼ï¸ Place ê³„ì‚° í”„ë ˆì„ ì €ì¥ ì™„ë£Œ: {save_path}")
    center_u, center_v, _ = find_red_center(frame)
    
    if center_u is None:
        print(f"ğŸ”´ ë¹¨ê°„ìƒ‰ ë¬¼ì²´ ë¯¸ê²€ì¶œ. ì¢Œí‘œ ê³„ì‚° ì¤‘ì§€.")
        return False, None

    # 2. ì˜¤ì°¨ ê³„ì‚° ë° MM ë³€í™˜
    delta_X_mm, delta_Y_mm, delta_u_pixel, delta_v_pixel = convert_pixel_to_robot_move(center_u, center_v)
    
    total_pixel_error = np.sqrt(delta_u_pixel**2 + delta_v_pixel**2)
    
    print(f"\n--- ğŸ¤– Vision-Guided ì •ë ¬ ê³„ì‚° (Single Shot) ---")
    print(f"  [Detect] í”½ì…€ ì˜¤ì°¨: {total_pixel_error:.2f}px (U: {delta_u_pixel}, V: {delta_v_pixel})")
    print(f"  [Move] í•„ìš”í•œ ì´ë™ëŸ‰: X:{delta_X_mm:.2f}mm, Y:{delta_Y_mm:.2f}mm")

    # 3. ìµœì¢… ëª©í‘œ ì¢Œí‘œ ê³„ì‚°
    final_place_coords = list(GLOBAL_TARGET_COORDS) # ê¸°ì¤€ ì¢Œí‘œ ë³µì‚¬
    
    # í”½ì…€ ì˜¤ì°¨ë¥¼ MMìœ¼ë¡œ ë³€í™˜í•œ ë§Œí¼ ë¡œë´‡ ì¢Œí‘œì— ì¶”ê°€í•˜ì—¬ 'ì •ë ¬ëœ' ëª©í‘œ ì¢Œí‘œë¥¼ ìƒì„±
    final_place_coords[0] += delta_X_mm # Xì¶• ì´ë™ ëª…ë ¹ ì ìš©
    final_place_coords[1] += delta_Y_mm # Yì¶• ì´ë™ ëª…ë ¹ ì ìš©
    
    # Zì¶• ë†’ì´ëŠ” ë¯¸ë¦¬ ì„¤ì •ëœ í”½ì—… ë†’ì´ë¡œ ê³ ì •
    final_place_coords[2] = PICK_Z_HEIGHT 

    print(f"âœ… ëª©í‘œ ì¢Œí‘œ ê³„ì‚° ì™„ë£Œ. ìµœì¢… ì¢Œí‘œ: X:{final_place_coords[0]:.2f}, Y:{final_place_coords[1]:.2f}, Z:{PICK_Z_HEIGHT:.2f}")
    
    # 4. ê³„ì‚°ëœ ì¢Œí‘œ ë°˜í™˜
    return True, final_place_coords

# ===============================================
# ğŸ§  AI/Vision ì •ë³´ ìˆ˜ì§‘ í•¨ìˆ˜ (ë¡œë´‡ ë™ì‘ ì œê±°)
# ===============================================

async def pick_data_collector(cap, ai_model):
    """
    ë¡œë´‡ì˜ Pick ë™ì‘ì— í•„ìš”í•œ ë¹„ì „/AI ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤. (ë¡œë´‡ ë™ì‘ ì œê±°)
    """
    global TEST_PICK_POSE_WIDTH, TEST_PICK_POSE_HEIGHT, transform

    # í˜„ì¬ í”„ë ˆì„ ìº¡ì²˜
    ret, frame = cap.read()
    if not ret:
        logger.error("âŒ ì¹´ë©”ë¼ í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨. ë°ì´í„° ìˆ˜ì§‘ ì¤‘ë‹¨.")
        return False, "Unknown", 0.0, [0.0]*6, None
    
    center_x, center_y, largest_contour, rect, cropped_img = find_object_center(frame)

    if rect is None:
        logger.error("âŒ ë¬¼ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° ìˆ˜ì§‘ ì¤‘ë‹¨.")
        return False, "Unknown", 0.0, [0.0]*6, None
    
    # AI ê°ì²´ ë¶„ë¥˜ ìˆ˜í–‰
    predicted_class, confidence = classify_object(ai_model, transform, cropped_img)
    print(f"\nğŸ§  AI ë¶„ë¥˜ ê²°ê³¼: **{predicted_class}** (ì‹ ë¢°ë„: {confidence*100:.2f}%)")

    (center_u, center_v), (w, h), angle = rect
    
    # í”½ì—… ìì„¸ ê²°ì • (ê°€ë¡œ/ì„¸ë¡œ)
    if w > h:
        target_pose = list(TEST_PICK_POSE_WIDTH)
        logger.info(f"ğŸ“ ë¬¼ì²´ ì¥ì¶•: ê°€ë¡œ. ìµœì¢… Pose: ë‘˜ ë‹¤ TEST_PICK_POSE_WIDTH ì„ íƒ.")
    else: 
        target_pose = list(TEST_PICK_POSE_WIDTH)
        logger.info(f"ğŸ“ ë¬¼ì²´ ì¥ì¶•: ì„¸ë¡œ. ìµœì¢… Pose: ë‘˜ ë‹¤ TEST_PICK_POSE_WIDTH ì„ íƒ.")
    
    # ì‚¬ìš©ì ì§€ì • crop í›„ serverë¡œ ì†¡ì‹ í•  ì´ë¯¸ì§€
    send_img = frame[30:400, 30:340]
    
    return True, predicted_class, confidence, target_pose, send_img


# ===============================================
# ğŸ“¡ OPC UA í†µì‹  í•¨ìˆ˜ (ê¸°ì¡´ê³¼ ë™ì¼)
# ===============================================

async def send_full_result(module_type: str, confidence: float, pick_coord: list, status: str, image_to_send: np.ndarray = None):
    """
    ë¶„ë¥˜, í”½ì—… ê²°ê³¼ ë° ë¯¸ì…˜ ìƒíƒœë¥¼ JSON í˜•íƒœë¡œ ë¬¶ì–´ OPC UA ì„œë²„ì— í•œ ë²ˆì— ì†¡ì‹ í•©ë‹ˆë‹¤.
    """
    global OPCUA_WRITE_URL, WRITE_OBJECT_NODE_ID, WRITE_METHOD_NODE_ID
    
    # --- ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ì¸ì½”ë”© ë¡œì§ (ê¸°ì¡´ê³¼ ë™ì¼) ---
    base64_img_str = ""
    if image_to_send is not None and status != "arm_mission_failure": # ì‹¤íŒ¨ ì‹œ ì´ë¯¸ì§€ë¥¼ ë³´ë‚´ì§€ ì•Šì•„ íŠ¸ë˜í”½ ì ˆì•½
        try:
            resized_img = cv2.resize(image_to_send, (224, 224), interpolation=cv2.INTER_AREA)
            encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 80] 
            _, buffer = cv2.imencode('.jpg', resized_img, encode_param)
            base64_img_bytes = base64.b64encode(buffer)
            base64_img_str = base64_img_bytes.decode('utf-8')
            logger.info(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ì¸ì½”ë”© ì™„ë£Œ. Base64 ë¬¸ìì—´ ê¸¸ì´: {len(base64_img_str)}")
        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ ì¸ì½”ë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            base64_img_str = ""
    
    # ğŸ“Œ í†µí•©ëœ JSON ë°ì´í„° êµ¬ì„± (status í•„ë“œ ì¶”ê°€)
    vision_result = {
        "module_type": module_type,
        "classification_confidence": confidence,
        "pick_coord": [f"{c:.2f}" for c in pick_coord],
        "pick_coord_confidence": 0.9984073221683503,
        "img": base64_img_str,
        "status": status # â¬…ï¸ ë¯¸ì…˜ ìƒíƒœ í†µí•©
    }
    json_str = json.dumps(vision_result)

    # ğŸš€ í´ë¼ì´ì–¸íŠ¸ ì „ì†¡ ë°ì´í„° í™•ì¸
    print("\n========================================================")
    print(f"ğŸš€ [FULL RESULT] í´ë¼ì´ì–¸íŠ¸ê°€ ì„œë²„ë¡œ ì „ì†¡í•˜ëŠ” ìµœì¢… í†µí•© JSON:")
    print(json_str)
    print("========================================================\n")
    
    try:
        async with AsyncuaClient(OPCUA_WRITE_URL) as client:
            obj = client.get_node(WRITE_OBJECT_NODE_ID)
            method_node = client.get_node(WRITE_METHOD_NODE_ID)
            json_variant = ua.Variant(json_str, ua.VariantType.String)

            print(f"[OPC UA WRITE - FULL_RESULT] call_method(Module: {module_type}, Status: {status}) (Method: {WRITE_METHOD_NODE_ID})")
            result_code, result_message = await obj.call_method(
                method_node.nodeid,
                json_variant
            )
            logger.info(f"OPC UA í†µí•© ê²°ê³¼ ì†¡ì‹  ì™„ë£Œ. ResultCode: {result_code}")

    except Exception as e:
        logger.error(f"OPC UA í†µí•© ê²°ê³¼ ì†¡ì‹  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def sync_flush_camera_buffer(cap, num_frames=10):
        for _ in range(num_frames):
            cap.read() # í”„ë ˆì„ì„ ì½ì–´ ë²„ë¦¼
            time.sleep(0.01)

class SubHandler:
    
    def __init__(self, mycobot_instance, camera_instance, ai_model_instance):
        self.mc = mycobot_instance
        self.cap = camera_instance
        self.ai_model = ai_model_instance
        logger.info("SubHandler ì´ˆê¸°í™” ì™„ë£Œ.")
        

    def datachange_notification(self, node, val, data):
        """ë°ì´í„° ë³€ê²½ ì•Œë¦¼ ì‹œ í˜¸ì¶œë˜ëŠ” ë¹„ë™ê¸°ì  ì½œë°± í•¨ìˆ˜"""
        asyncio.create_task(self.execute_command_and_respond(val))
    
    async def execute_command_and_respond(self, val):
        """
        [ìµœì¢… ìˆ˜ì •] ëª…ë ¹ì„ íŒŒì‹±í•˜ê³  MyCobot ë™ì‘ì„ ëª¨ë‘ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        vision-guided place ì´ë™ ëª…ë ¹ê¹Œì§€ ì—¬ê¸°ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        """
        global SEQUENTIAL_MOVE_DELAY, MOVEMENT_SPEED, GRIPPER_OPEN_VALUE, GRIPPER_CLOSED_VALUE, GRIPPER_SPEED, GRIPPER_ACTION_DELAY

        print(f"\nâ– â–¡â– â–¡â– â–¡â– â–¡â– â–¡â– â–¡[OPC UA READ] ìˆ˜ì‹  ê°’: {val}")

        command = None
        if isinstance(val, str):
            try:
                json_data = json.loads(val)
                if "move_command" in json_data:
                    command = json_data["move_command"]
            except json.JSONDecodeError:
                command = val # Ready ê°™ì€ ì¼ë°˜ ë¬¸ìì—´ë„ commandë¡œ ê°„ì£¼

        if not command or self.mc is None or self.cap is None:
            logger.warning(f"-> ë¡œë´‡/ì¹´ë©”ë¼ ì—°ê²° ë¬¸ì œ ë˜ëŠ” ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹: {command}")
            return
        
        # 3. MyCobot ë™ì‘ ìˆ˜í–‰ ë° ì‘ë‹µ
        if command == "go_home":
            logger.info("-> MyCobot: go_home ëª…ë ¹ ìˆ˜í–‰ (CONVEYOR_CAPTURE_POSEë¡œ ì´ë™)")
            
            # ì¤‘ê°„ í¬ì¦ˆë¡œ ì´ë™
            await asyncio.to_thread(self.mc.send_coords, INTERMEDIATE_POSE_ANGLES, MOVEMENT_SPEED)
            await wait_until_stopped(self.mc)
            
            # ì»¨ë² ì´ì–´ ìº¡ì²˜ í¬ì¦ˆë¡œ ì´ë™
            await asyncio.to_thread(self.mc.send_angles, CONVEYOR_CAPTURE_POSE, MOVEMENT_SPEED)
            await wait_until_stopped(self.mc)
            logger.info("âœ… go_home (CONVEYOR_CAPTURE_POSE) ì´ë™ ì™„ë£Œ.")
        
        elif command == "mission_start":
            logger.info("-> MyCobot: mission_start ëª…ë ¹ ìˆ˜í–‰ (Vision-Guided Pick ì‹œì‘)")
            
            # 1. Vision/AI ë°ì´í„° ìˆ˜ì§‘ (Pick ëª©í‘œ ì¢Œí‘œ)
            success, module_type, confidence, target_pick_pose, send_img = await pick_data_collector(self.cap, self.ai_model)

            if not success:
                logger.error("âŒ ë°ì´í„° ìˆ˜ì§‘/ë¬¼ì²´ ê²€ì¶œ ì‹¤íŒ¨. OPC UA ì‹¤íŒ¨ ë³´ê³  ì†¡ì‹ .")
                await send_full_result(
                    module_type=module_type, confidence=confidence, 
                    pick_coord=target_pick_pose, status="arm_mission_failure"
                )
                return 

            # 2. OPC UA ê²°ê³¼ ì „ì†¡ (ë¡œë´‡ ë™ì‘ ì§ì „ì— ì •ë³´ ì „ì†¡)
            await send_full_result(
                module_type=module_type, confidence=confidence, 
                pick_coord=target_pick_pose, status="arm_mission_success", 
                image_to_send=send_img
            )

            # 3. í”½ì—… ë™ì‘ ì‹œí€€ìŠ¤ ì‹œì‘
            safe_pick_pose = list(target_pick_pose)
            safe_pick_pose[2] += 50 
            
            logger.info(f"â¬†ï¸ ì•ˆì „ í¬ì¦ˆë¡œ ì´ë™: Z:{safe_pick_pose[2]:.2f}")
            await asyncio.to_thread(self.mc.send_coords, safe_pick_pose, MOVEMENT_SPEED)
            await wait_until_stopped(self.mc)

            logger.info(f"\nâ¬‡ï¸ í”½ì—… ì‹œì‘: Z:{target_pick_pose[2]:.2f} í•˜ê°•.")
            await asyncio.to_thread(self.mc.send_coords, target_pick_pose, MOVEMENT_SPEED - 30)
            await wait_until_stopped(self.mc)
            
            await asyncio.to_thread(self.mc.set_gripper_value, GRIPPER_CLOSED_VALUE, GRIPPER_SPEED)
            await asyncio.sleep(GRIPPER_ACTION_DELAY)
            logger.info("âœ… ê·¸ë¦¬í¼ ë‹«ê¸° ì™„ë£Œ (Pick).")

            # 4. ì¤‘ê°„ í¬ì¦ˆ ì´ë™ (Place ì¤€ë¹„)
            await asyncio.to_thread(self.mc.send_angles, CONVEYOR_CAPTURE_POSE, MOVEMENT_SPEED)
            await wait_until_stopped(self.mc)
            logger.info("âœ… CONVEYOR_CAPTURE_POSE ì´ë™ ì™„ë£Œ.")

            await asyncio.to_thread(self.mc.send_angles, ROBOTARM_CAPTURE_POSE, MOVEMENT_SPEED)
            await wait_until_stopped(self.mc)
            logger.info("âœ… ROBOTARM_CAPTURE_POSE ì´ë™ ì™„ë£Œ.")
            
            await asyncio.to_thread(sync_flush_camera_buffer, self.cap, 15)
            await asyncio.sleep(0.5)

            # 5. Place ëª©í‘œ ì¢Œí‘œ ê³„ì‚° (Vision-Guided)
            print(f"\nğŸš€ Place ì‘ì—… ì‹œì‘: Vision-Guided ëª©í‘œ ì¢Œí‘œ ê³„ì‚° ì‹œì‘")
            place_calc_success, final_place_coords = await place_coords_calculator(self.cap)
            
            if not place_calc_success:
                logger.error("âŒ Place ëª©í‘œ ì¢Œí‘œ ê³„ì‚° ì‹¤íŒ¨. Place ë™ì‘ ì¤‘ë‹¨.")
                # ë¯¸ì…˜ ì‹¤íŒ¨ë¡œ ì²˜ë¦¬í•˜ì§€ ì•Šê³ , í˜„ì¬ ìœ„ì¹˜ì—ì„œ ê·¸ë¦¬í¼ë¥¼ ì—¬ëŠ” ë“± ì•ˆì „ ì¡°ì¹˜ í•„ìš”
                await asyncio.to_thread(self.mc.send_angles, ROBOTARM_CAPTURE_POSE, MOVEMENT_SPEED) # ì•ˆì „ í¬ì¦ˆë¡œ ë³µê·€
                await wait_until_stopped(self.mc)
                return 
            
            # 6. Place ë™ì‘ ì‹¤í–‰
            
            # Place ëª©í‘œì˜ ì•ˆì „ í¬ì¦ˆ (Zì¶• + 50)
            safe_place_coords = list(GLOBAL_TARGET_TMP_COORDS)
            
            # ì•ˆì „ í¬ì¦ˆë¡œ ì´ë™
            logger.info(f"â¬†ï¸ Place ì•ˆì „ í¬ì¦ˆë¡œ ì´ë™: X:{safe_place_coords[0]:.2f}, Y:{safe_place_coords[1]:.2f} (Z:{safe_place_coords[2]:.2f})")
            print(safe_place_coords)
            await asyncio.to_thread(self.mc.send_coords, safe_place_coords, MOVEMENT_SPEED - 30)
            await wait_until_stopped(self.mc)

            # Place ì§€ì ìœ¼ë¡œ í•˜ê°•
            logger.info(f"â¬‡ï¸ Place ì§€ì ìœ¼ë¡œ í•˜ê°•: X:{final_place_coords[0]:.2f}, Y:{final_place_coords[1]:.2f} (Z:{final_place_coords[2]:.2f})")
            print(final_place_coords)
            await asyncio.to_thread(self.mc.send_coords, final_place_coords, MOVEMENT_SPEED - 30)
            await wait_until_stopped(self.mc)

            print("âœŠ ê·¸ë¦¬í¼ ì—¬ëŠ” ì¤‘ (Place ë™ì‘)...")
            # ê·¸ë¦¬í¼ ì—´ê¸° (Place)
            await asyncio.to_thread(self.mc.set_gripper_value, GRIPPER_OPEN_VALUE, GRIPPER_SPEED)
            await asyncio.sleep(GRIPPER_ACTION_DELAY)
            print(f"âœ… Place ì™„ë£Œ (ê·¸ë¦¬í¼ ì—´ë¦¼).")

            # Place ì™„ë£Œ í›„ ì•ˆì „ í¬ì¦ˆë¡œ ë³µê·€
            await asyncio.to_thread(self.mc.send_coords, safe_place_coords, MOVEMENT_SPEED)
            await wait_until_stopped(self.mc)
            
        elif command == "Ready":
            logger.info("-> MyCobot: Ready ìƒíƒœ ìˆ˜ì‹ , ëŒ€ê¸° ì¤‘...")
            
        else:
            logger.warning(f"-> MyCobot: ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹: {command}")

async def arm_subscriber():
    """ OPC UA í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‹¤í–‰í•˜ê³  êµ¬ë…ì„ ì„¤ì •í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜ """
    global mc, cap, ai_model

    # ğŸ“Œ 1. AI ëª¨ë¸ ë¡œë“œ
    ai_model = load_model(MODEL_WEIGHTS_PATH, NUM_CLASSES)
    
    # ğŸ“Œ 2. MyCobot ì—°ê²° ì´ˆê¸°í™”
    try:
        mc = MyCobot320(PORT, BAUD)
        mc.set_color(0, 0, 255) 
        logger.info(f"MyCobot320ì´ {PORT}ì— {BAUD} ì†ë„ë¡œ ì„±ê³µì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ê·¸ë¦¬í¼ ì´ˆê¸°í™” ë¡œì§
        mc.set_gripper_mode(0)
        mc.init_electric_gripper()
        await asyncio.sleep(2)
        mc.set_electric_gripper(0)
        mc.set_gripper_value(GRIPPER_OPEN_VALUE, GRIPPER_SPEED, 1) # GRIPPER_OPEN_VALUE (85)ë¡œ ì—´ë¦¼
        await asyncio.sleep(2)
        logger.info(f"-> MyCobot320: ì „ê¸° ê·¸ë¦¬í¼ ì´ˆê¸°í™” ì™„ë£Œ ({GRIPPER_OPEN_VALUE} ìœ„ì¹˜ë¡œ ì´ë™).")
        
    except Exception as e:
        logger.error(f"MyCobot320 ì—°ê²° ë˜ëŠ” ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        mc = None

    # ğŸ“Œ 3. ì¹´ë©”ë¼ ì—°ê²° ì´ˆê¸°í™”
    cap = cv2.VideoCapture(CAMERA_INDEX, cv2.CAP_DSHOW)
    if not cap.isOpened():
        logger.error(f"ì¹´ë©”ë¼ ì¸ë±ìŠ¤ {CAMERA_INDEX}ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        cap = None

    if mc is None or cap is None:
        logger.error("MyCobot ë˜ëŠ” ì¹´ë©”ë¼ ì—°ê²° ë¬¸ì œë¡œ Vision-Pick ë¯¸ì…˜ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        if mc is not None:
            mc.close()
        return # ì—°ê²° ì‹¤íŒ¨ ì‹œ ì¢…ë£Œ

    # ğŸ“Œ 4. OPC UA ì—°ê²° ë° êµ¬ë… ì„¤ì •
    logger.info(f"OPC UA ìˆ˜ì‹  ì„œë²„ì— ì—°ê²° ì‹œë„: {OPCUA_READ_URL}")

    try:
        async with AsyncuaClient(OPCUA_READ_URL) as client:
            logger.info("OPC UA ìˆ˜ì‹  ì„œë²„ì— ì„±ê³µì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")

            handler = SubHandler(mc, cap, ai_model)
            # êµ¬ë… ê°„ê²© 100ms
            sub = await client.create_subscription(100, handler)

            cmd_node = client.get_node(READ_METHOD_NODE_ID) 

            await sub.subscribe_data_change(cmd_node)
            logger.info(f"ë…¸ë“œ '{READ_METHOD_NODE_ID}' êµ¬ë… ì‹œì‘. ë°ì´í„° ìˆ˜ì‹  ëŒ€ê¸° ì¤‘...")
            while True:
                await asyncio.sleep(1) # í´ë¼ì´ì–¸íŠ¸ ìœ ì§€
    
    except Exception as e:
        logger.error(f"OPC UA ì—°ê²° ë˜ëŠ” êµ¬ë… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        # ğŸ“Œ 5. ìì› í•´ì œ
        if mc is not None:
            mc.set_color(0, 0, 0)
            mc.close()
            logger.info("MyCobot ì •ë¦¬ ì™„ë£Œ.")
        if cap is not None:
            cap.release()
            cv2.destroyAllWindows()
            logger.info("ì¹´ë©”ë¼ ì •ë¦¬ ì™„ë£Œ.")
        logger.info("OPC UA í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ.")


if __name__ == "__main__":
    try:
        # ë¹„ë™ê¸° ë©”ì¸ ë£¨í”„ ì‹¤í–‰
        asyncio.run(arm_subscriber())
    except KeyboardInterrupt:
        logger.info("ì‚¬ìš©ì ì¤‘ë‹¨ (Ctrl+C). í”„ë¡œê·¸ë¨ ì¢…ë£Œ.")
    except Exception as e:
        logger.critical(f"í”„ë¡œê·¸ë¨ ìµœì¢… ì˜¤ë¥˜: {e}")